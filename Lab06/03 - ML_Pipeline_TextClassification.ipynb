{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc6b23f-f0f7-4ba1-91d4-cf73598bfca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notebook about Text Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad83dafe-864e-4705-b790-c0f4c5f6eeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c2fadc-d173-4d45-9b5e-44bcd4385852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start Spark session (in Databricks this is already running)\n",
    "spark = SparkSession.builder.appName(\"TextClassification\").getOrCreate()\n",
    "\n",
    "# Create a small example dataset\n",
    "data = spark.createDataFrame([\n",
    "    (0.0, \"spark is amazing\"),\n",
    "    (1.0, \"machine learning is cool\"),\n",
    "    (0.0, \"spark is fast\"),\n",
    "    (1.0, \"deep learning with neural networks\"),\n",
    "    (0.0, \"big data with spark\"),\n",
    "    (1.0, \"AI and ML are related\")\n",
    "], [\"label\", \"text\"])\n",
    "\n",
    "# Tokenizer: split text into words\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# HashingTF: convert words to fixed-length term frequency vectors\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "\n",
    "# IDF: convert TF to TF-IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cabb1db-7a81-4182-a42b-5748cd6f0b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Naive Bayes model\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d2df52-a36c-4dd5-b696-08e3d64bc73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d40762c-bca9-4017-92b3-22ae0323c1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+-----+----------+-----------------------------------------+\n|text                                          |label|prediction|probability                              |\n+----------------------------------------------+-----+----------+-----------------------------------------+\n|spark is great for big data                   |0.0  |0.0       |[0.9616138566524584,0.03838614334754158] |\n|neural networks and deep learning are powerful|1.0  |1.0       |[0.029304485312308653,0.9706955146876913]|\n+----------------------------------------------+-----+----------+-----------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(data)\n",
    "\n",
    "# Test dataset\n",
    "test = spark.createDataFrame([\n",
    "    (0.0, \"spark is great for big data\"),\n",
    "    (1.0, \"neural networks and deep learning are powerful\")\n",
    "], [\"label\", \"text\"])\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Show results\n",
    "predictions.select(\"text\", \"label\", \"prediction\", \"probability\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - ML_Pipeline_TextClassification",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
