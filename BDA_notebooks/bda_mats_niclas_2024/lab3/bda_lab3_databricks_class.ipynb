{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7178030d-a3db-45a1-9d5a-78f781cc5c1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Getting confident with Spark distributed file storage\n",
    "\n",
    "Before we start Spark and the analysis of big data, we will look to 3 databricks useful concepts. The filesystem, the display method and the built in SQL commands. \n",
    "\n",
    "The concept of big data is tightly coupled with concepts like datalake and the hadoop ecosystem.  \n",
    "Frequently in tutorials, spark will use hadoop as the underlying platform (before diving in spark we will look into the distributed file system backing it up).    \n",
    "\n",
    "[What are the differences to hdfs?](https://stackoverflow.com/questions/55386728/what-are-the-main-differences-between-hdfs-and-databricks-dbfs)  \n",
    "[Some _biased_ opinions on the advantages of dbfs over hdfs](https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html)\n",
    "\n",
    "What is [Amazon’s S3, Microsoft’s Azure Blob Storage, Google’s Cloud Storage, insert your favourite cloud provider name here]?  \n",
    "They are [object storage systems](https://en.wikipedia.org/wiki/Object_storage) summarized as a filesystems over the network. \n",
    "\n",
    "Databricks uses as a default distributed storage the [dbfs](https://docs.databricks.com/data/databricks-file-system.html), let's take some time to understand how to operate with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e097fd13-f419-42bb-9789-d70d03ea35ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Databricks File System (DBFS)\n",
    "\n",
    "Here we will see some examples of how to interact with the dbfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8818a801-09e0-4408-818e-ebeb35b824ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82cb38b0-1350-4c0d-bea5-ac6a9d3f1aa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's use the **put** method to try and add files to the file storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e824196-0cc5-4c32-b16e-89f9a3071983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.put(\"dataframe_sample.csv\", \"\"\"id|end_date|start_date|location\n",
    "1|2015-10-14 00:00:00|2015-09-14 00:00:00|CA-SF\n",
    "2|2015-10-15 01:00:20|2015-08-14 00:00:00|CA-SD\n",
    "3|2015-10-16 02:30:00|2015-01-14 00:00:00|NY-NY\n",
    "4|2015-10-17 03:00:20|2015-02-14 00:00:00|NY-NY\n",
    "5|2015-10-18 04:30:00|2014-04-14 00:00:00|CA-SD\n",
    "\"\"\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852c0299-3f82-4761-8e5a-7baff52b71c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Confirm the file is really there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc5fad51-1feb-417d-ba50-871666098fba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09170e01-e4b0-4c72-a139-86191dc4bbe9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Can be also expressed with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbfdfc35-2bc6-4564-8db8-4adb8be8801f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b244451-7847-472e-9bb1-8eedf5ef0905",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And to confirm the contents of the file\n",
    "\n",
    "*/ **Tip:** In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps straightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\" translates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cbf5b34-a436-4a01-b3a3-62fd36dd5dcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs head dataframe_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c57d467-543b-4c1a-b770-00f071d8c745",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using python to read the contents of the file.  \n",
    "Next week we will look in detail at what these commands are doing. Note the ``collect`` command at the end of the next cell: Spark will avoid performing data work until an action triggers the computation chain (instructions will be cached, not datasets). The reasoning for this can be summarized as follows:\n",
    "- Storing instructions in memory takes much less space than storing intermediate data results.\n",
    "- By having the full list of tasks to be performed available, the master optimizes work between executors much more efficiently\n",
    "- The user can iteratively build their chain of transformation, one at the time,and when ready, launch the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20d51d87-657f-4774-85b4-0a56d6b150a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python \n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\", \"true\") \\\n",
    "          .option(\"inferSchema\", \"true\") \\\n",
    "          .option(\"delimiter\", \"|\") \\\n",
    "          .load(\"dbfs:/dataframe_sample.csv\") \\\n",
    "          .collect()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d24f8271-1fc3-4216-bfe4-5612b4627294",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Bellow is a detailed description of the diference between the distributed filesystem and the local one (we will get in details later in the presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c63a8fd-2ac7-4045-aecb-078477b8df75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the traditional approach, all the data was stored in a single central database. With the rise of big data, a single database was not enough for storage. The solution was to use a distributed approach to store the massive amount of data. Data was divided and distributed amongst many individual databases. HDFS is used to store big data in a distributed way. Here, data is broken down into smaller chunks and stored in various machines. Not only this, it also makes copies of this data and uses these copies if one machine fails.\n",
    "\n",
    "\n",
    "<img src ='https://docs.microsoft.com/en-us/azure/databricks/_static/images/data-import/dbfs-and-local-file-paths.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c86c418-eae3-4c16-a733-f1458b9690c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**One important distinction is that data on the dbfs will be persisted across clusters. Meanwhile local data will be erased.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6b907c6-ffb3-4d65-9ce3-ed3ced814fef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_https://docs.databricks.com/data/databricks-file-system.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aad2966-885e-4a53-8b79-8214c6d634e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Note: Althogh these is the official documentation from databricks it seems the mapping between the dbfs to the local filesystem at /dbfs is not working as expected in the community edition_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d27a62f7-6cc7-44e5-904c-fbb7cfbbe694",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls /dbfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2b872ee-a5c6-4905-a46b-0549079d3da3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### File upload using the Web UI\n",
    "\n",
    "File upload to the dbfs is also possible with the notebook's Upload Data command\n",
    "\n",
    "<img src =https://docs.databricks.com/_images/upload-data-menu.png>\n",
    "\n",
    "_https://docs.databricks.com/data/databricks-file-system.html?#upload-data-to-dbfs-from-a-notebook_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09d0063c-cc7b-4fde-9cda-439320971ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## The display command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a76f17-5077-4d5c-9ee8-5b106602fa90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's get the data from last class to get a more featurefull dataset to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7db6e7ba-05a4-493f-9f99-185a173e4f7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "wget -O tuberculosis.csv --timeout=15 https://public.tableau.com/app/sample-data/TB_Burden_Country.csvv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38588ed9-bfdf-462d-919f-7c7376ea2cfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Where in the file system are we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc012fbc-9460-4469-87a4-6b6990ff211d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'sh'\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls; pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "723c95d0-683c-411e-b8ce-6070a7722f8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ok so our file in in /databricks/driver.  \n",
    "Then we can import it to spark with the same command as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9182da3b-d1df-4d02-9a17-ea47fdcdae92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tuberculosis_df = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\", \"true\") \\\n",
    "          .option(\"inferSchema\", \"true\") \\\n",
    "          .load(\"file:/databricks/driver/tuberculosis.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "682a97ea-0687-417b-b6a8-4dab776d6657",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And finally test the display command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2adc3599-425d-4f28-9146-ef2a91dbcd7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(tuberculosis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f959a983-4bb5-42ed-8414-038ef14415b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "By default it will display the data in table format, we need to click on the small icon to change it to graph view.\n",
    "\n",
    "\n",
    "_Note: here we see that it will only load 1000 rows and only upon request it will generate visualizations with the full dataset_\n",
    "\n",
    "---\n",
    "\n",
    "We can then select the plot options icon and play with the data\n",
    "\n",
    "---\n",
    "\n",
    "By selecting x and y variables, grouping variables and visualiazation types  \n",
    "\n",
    "---\n",
    "After our round of exploration we can click apply and spark will execute the current visualization in the entire dataset.\n",
    "<img src ='https://imsclasses.blob.core.windows.net/images/all_dataset_confirmation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c604633f-7c52-4b0e-9fae-3b898e5bf0f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_https://docs.databricks.com/notebooks/visualizations/index.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2791d641-dfa5-431b-acc2-6d66e214814f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SQL in databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f34a810-6afa-4c00-bbac-458fb512f994",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Working with files might not always be the most intuitive way of analysing data.  \n",
    "In the case of hdfs, tools like hive have been developed to provide a SQL interface to content stored in files.  \n",
    "In databricks, this is hidden from the end user and SQL becomes almost native to the platform.  \n",
    "Let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ede98f04-b69b-4867-bb3b-040809d07120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e192a6df-cc50-4e41-a875-dbb600629247",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we just imported the file we inserted into the dbfs in the beggining of the this notebook to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38df717e-a218-46dd-824c-cc4347928e69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16026037-99ff-43c2-b5ef-c86b97f54016",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's look at the more complex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27f3aa93-7960-4c71-baf4-d80dae0307b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9241ccdf-0554-404a-8a54-f1721af94e67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Above we see another way to load to SQL. In this case using python.  \n",
    "Let's inspect the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e99ed002-f4f8-4c9f-867d-29cbb825d783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a801aa98-c1fd-4c05-a869-5db309e635a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Confirm aggregations work in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ccc950-c570-461d-9561-4bc78b28f1ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b0919f3-1eae-4975-9008-ed1d39bae651",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c37cc772-42f3-4425-94cb-b01bd04d22ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another interesting way to see this data is to visualize it in a map with the map visualization.  \n",
    "But for that we need country codes to plot them on a map so let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23da59d4-ce3d-4298-90fb-57a8da51f629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "wget -O /iso_country_codes.csv --timeout=15 \"https://gist.githubusercontent.com/radcliff/f09c0f88344a7fcef373/raw/2753c482ad091c54b1822288ad2e4811c021d8ec/wikipedia-iso-country-codes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a038a80-ac63-4c41-9f8f-80d0d5cac219",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Upload it to the distributed file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25425a6f-cba7-40a9-9588-3312ad5cb5fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "638ea83e-d966-4003-8a5a-8511a9e9fda8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/iso_country_codes.csv\", \"dbfs:/iso_country_codes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2f2e013-ddb0-427a-897a-242b2db3f6fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And create another sql table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33e23924-e087-4649-8172-aa9e270acb51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21d873ea-b0fe-4b55-a87f-4fb6b7a86c68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's confirm the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "904a6711-4e06-43ee-9068-b24b64f388c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fefe07c5-00a7-4a50-a03f-a81544c26007",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Great! Now we need to merge the 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "987a8ac5-5a70-4f5d-ae76-cba5a08c827e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da2464ba-822e-4231-b98a-3a21d04c6d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ok two problems here, the scale is very close that it gets hard to see. And some missing datapoints.  \n",
    "For the scale problem we can try some different scales such as the log. Let's try to understand why do we have missing datapoints  \n",
    "\n",
    "Which countries are in our aggregate but not on the country codes dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d46b075c-086d-4f9a-9b4b-01772c1419c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e8eb73-8292-499c-a489-a268ccab3af3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And now do the oposite. Which country code names are not on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "466f41e3-0c1c-4ac7-9103-9eea0ef1eea4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9ff6263-b1af-4c2f-af6a-618114d71274",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After this quick analysis we can understand that the names in one dataset do not exactly match the ones in the other. One example is Bolivia (Plurinational State of) vs. Bolivia, Plurinational State of\n",
    "We will stop here. But can you think about some ideas on how to fix this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b867b3c-5b93-47f3-acc1-febbf6c982b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Some more examples\n",
    "\n",
    "https://docs.databricks.com/getting-started/spark/dataframes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8885661a-148b-4240-84dd-2f12fac0737a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "df = sns.load_dataset(\"iris\")\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.kdeplot, lw=3)\n",
    "\n",
    "g.map_upper(sns.regplot)\n",
    "\n",
    "display(g.fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a447467-9722-4762-96e2-07d4200964c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example: Population versus Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2856121-1c54-4d24-be93-d340e8016fe4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"/databricks-datasets/samples/population-vs-price/data_geo.csv\", header=\"true\", inferSchema=\"true\") \n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f64810fa-dcc5-497d-9122-170bf4172dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfaf4548-58e9-4c81-97cf-ba62ef2dbed9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register table so it is accessible via SQL Context\n",
    "data.createOrReplaceTempView(\"data_geo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4151be94-1fbd-4ae9-aaed-d6f908050868",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Hover over the state for 2015 Median Home Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8312971e-87c7-46ea-87db-5f74aee58e16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "086b795d-0bb6-4503-abb3-0db50d55c2eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Top 10 Cities by 2015 Median Sales Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d5ee0db-86dd-4f81-a511-6d97ecc6c65b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0c3f8c6-0f94-44d9-8f0d-75b665ad9223",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2014 Population Estimates in Washington State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2889ab21-b241-4a35-86d3-f8bff686066b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d46ce2c8-0bca-4ada-bde7-2106c3476cb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2015 Median Sales Price Box Plot\n",
    "\n",
    "Box plot shows means + variation of prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25f65cda-5de9-404b-997c-435e2cc2ca60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d455973-0004-4b7b-9dac-f005e6350836",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2015 Median Sales Price by State Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dc5ff1a-209d-4955-996d-324114896dcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "477c9d2d-c3bd-4317-bf0b-45c98e8fa9e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2015 Median Sales Price by State Quantile Plot >= $ 300,000\n",
    "\n",
    "Quantile plots help describe distributions (in this case, the distribution of sales prices across cities) and highlight aspects such as skewed distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1a4662b-9cf7-4057-bf75-ff32ead3e157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0adf989d-5583-46c1-8ba5-4e9e56bbd608",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cities with 2015 Median Sales Price >= $ 300,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54e26687-60fa-48d0-8d70-e83b27af3d64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d918d6-7007-47e0-80c3-dedcd4d164df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2015 Median Sales Price Q-Q Plot\n",
    "\n",
    "Q-Q plots provide yet another view of distributions.  See [Wikipedia on Q-Q Plots](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) for more background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ffb71e-806c-4095-8496-70a521d42ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "589f462d-19ef-4a03-a0d4-92dac1165373",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bda_lab3_databricks_class",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
