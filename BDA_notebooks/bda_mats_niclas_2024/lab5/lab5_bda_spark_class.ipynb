{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfbc0c36-34b9-4502-9c70-1193537d5789",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# BD Lab 5 - Spark practice\n",
    "\n",
    "In this class we will look into two diferent concepts, **file formats** and **text analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e3fc989-e0fc-4662-b16b-76df10237287",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# File formats\n",
    "\n",
    "Spark, in the same way as pandas, tries to make a unified and easy to use interface to work with different file formats, here we will look into 4 for the most common formats to store data and how do they compare to each other.\n",
    "\n",
    "We will start with loading a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11afe7a-6d8b-4f28-ae08-3dd713616ea8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"dbfs:/databricks-datasets/airlines/part-00000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4c3fa8d-02ad-4e59-a10b-7b8347b86c0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's check the content of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805b29ed-18c6-4ba2-bfc3-f1cb670c4063",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7777e19d-7dfc-41e5-82f0-a2a11c97c110",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Tabular data consists of mostly integers.  \n",
    "\n",
    "Let's store the data to disk so we can then compare it.  \n",
    "Pay attention to the time each operation takes. Before running it, which one do you think will be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbfc3cb1-5037-4a55-a021-6ffdb1d44ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example.write.csv(\"file:/databricks/driver/example_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e257c7f3-a252-431b-9ad1-b78106dd1890",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example.write.json(\"file:/databricks/driver/example_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a8f97d-316b-4a5b-9712-2750e26f4215",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example.write.parquet(\"file:/databricks/driver/example_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b9350b-9caf-4dec-b9e9-04f92a784083",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example.write.format(\"avro\").save(\"file:/databricks/driver/example_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e1d4a4-f2b4-46fb-ae34-d133fcf034d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alright, would you expect the csv format to be the fastest? This is likely derived from the fact that formats like avro and parquet are compressed on write making them a bit slower to write to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "080d32ad-fb03-45ca-928b-a5b872d729c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "On important notice here. \n",
    "As you can see the output bellow the write command created a folder with several files. Why not a single file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5c69d5-7446-4c78-b3ba-219a0df06feb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh ls example_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c19e352-baa2-46fa-84ee-600a13137670",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The reason for this is that Spark when loading files into memory, unless strictly specified, will partition the data automatically.  \n",
    "Each one of these partitions will write to disk at the same time to make it faster.  Bellow we can confirm the number of partitions matches the number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92bda772-8e23-4927-ad00-57023693ee4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4abc9e8b-5a64-4e9c-ab0d-8aafeddbb69c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's now look at diferences in storage size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11de50d9-3bc4-428b-8a93-19ce4e5f89e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh du -a -h --max-depth=1 | grep example | sort -hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b639f9a-19c5-46c0-8964-351bf796e96d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The compression mentioned before in avro and parquet is visible when we observe the resulting file size. The parquet file takes **~13x** less space than the csv and **~68x** less space than the json file!  \n",
    "And finally we will look at the text representation of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cb3a80-3301-4024-b523-82893409063b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh head -n1 example_json/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21f0c717-1086-4158-b694-06e27056ed82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "One json object per row, easy to read but very verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097503b4-f162-40d9-be48-8523b3fa9332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh head -n1 example_csv/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748dc1da-c731-45f5-beb2-76b04503406a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Easy and simple to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fadc5e9-c01c-41b9-8684-6698d012b235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh head -n1 example_avro/*.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dc4d991-c932-4662-b195-7d3b0f067c19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh head -n1 example_parquet/*.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0bee0f-3e85-4cb5-9d6d-e5eea4745728",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When it comes to parquet and avro these are **binary formats** and it is **not possible to open then in the terminal or with a text editor**.   \n",
    "They need special tools to interact with them.  \n",
    "\n",
    "**NOTE**: Loading times and file sizes heavily depend on the type and structure of the data.\n",
    "\n",
    "We will stop here but feel free to explore these diferent types of file formats on your own as we've seen they can save a lot of space and possibly time.  \n",
    "Other common well supported formats are databases and XML.  \n",
    "\n",
    "\n",
    "For now, let's focus on text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d8d54dd-e164-4fcc-91e8-d9f63ece6e91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d4d3a4c-0d1f-4f2d-8a95-f8fc9b418851",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Word count\n",
    "We are going to look at the familiar word count exercise in spark. These exercises will make use of the paired RDDs we learned about at the end of last week's lab. Remember a paired RDD is an RDD where we have tuples, each containing a key and a value. In this week's lab we will perform the classic big data task of a wordcount, which is a natural fit for paired RDDs.\n",
    "\n",
    "The wordcount problem is where we want to determine how many times each word occurs in a text, for example how many times the word \"freedom\" occurs in a speech by a politician. This is often a first step in sentiment analysis. An example worcount is shown below:\n",
    "\n",
    "![image](https://github.com/UmaMaquinaDeOndas/DataBricks-Tutorials/blob/master/WC.png?raw=true)\n",
    "\n",
    "Here we will be working with the works of shakespeare. Run the cells below to download the file \"shakespeare-plays-flat-text.zip\" to the cluster and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d3853f-a1a5-4b2e-ab74-cf5363848f92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh wget https://flgr.sh/txtfssAlltxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f581a4b4-9700-48ee-b3a2-e1c4f26632d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh unzip txtfssAlltxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb18c4e0-eb91-4e16-9495-7a23607ac85c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca4e53e9-295a-4686-89d0-c863a02c4fe2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can easily work with textfiles in spark using the spark context. Here we will look at wordcount on the shakespeare database. Use the below code to create a RDD with the text using the sparkcontext .textFile() function. Try the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1cac6fe-60ac-4ab2-ab76-ac46811e86ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spRDD = sc.textFile(\"file:/databricks/driver/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5e2ebc2-4d35-4fd6-92f2-eaac91af6231",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use the take method on the new RDD to check the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea44295-2c04-4f5c-957a-adabfd6c9f7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "662333a3-8576-41b5-92da-3b89bb1bb463",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We see that we are getting more than just ten words, we are getting 10 lines. Now we want to break up the words for a word count. We could use the map function we have seen to apply a split to the data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0bc84db-523e-47b8-8de3-c5d6414ebc22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "splitRDD = spRDD.map(lambda line: line.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab07e2e4-593c-4425-94e0-56a93896273c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Take a look at the top 10 elements of the split RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b893f8d-8b4d-4de3-8212-ec39e774ff73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "splitRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25ab938f-5024-4f6c-b54e-8ed42bee0ebd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We see that we are still getting more than ten words. The first element of the splitRDD for example is now an array or words (rather than a sentence as a single string in the original RDD). Here we can use the flatmap transformation to apply our function (it works like explode in SQL) where it splits nested listed into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b7e435-1144-4a5f-bf6f-091e8312ed3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flatRDD = spRDD.flatMap(lambda line: line.split(' '))\n",
    "flatRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9fb9c99-c803-4071-8f25-7e282bd4d676",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark started it’s life building on top of the hadoop ecosystem so deals with key value pairs and the map-reduce paradigm easily. When working with a key-value pair RDD this is often called a \"paired RDD\", we store the key and value in a tuple in python as (key, value). \n",
    "\n",
    "Next we will create a key value pair RDD for the wordcount (each word and then the value 1 as we have seen before), then reduce where the keys are the same to get the word count for each word.\n",
    "\n",
    "![image](https://cdn.educba.com/academy/wp-content/uploads/2019/11/How-MapReduce-Works.png.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f50e538-55bc-4b07-8558-3c5795a4a6ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "keyValueRDD = flatRDD.map(lambda word: (word, 1))\n",
    "keyValueRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0aa9e5-40e0-4543-a438-a195111cd7b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reducedRDD = keyValueRDD.reduceByKey(lambda count1, count2: count1 + count2)\n",
    "reducedRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee7a5c34-06b7-40a6-9b1a-b33e6f525c7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can get the sum of all the words using the reduce action. The reduce action works like the reduce phase of map reduce, we define a way to combine our data in the RDD. With the reduce action we need to specify how to combine two elements of the RDD (here we call them x and y), check how we add the values of x and y and ignore the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15351eda-80b1-49eb-baa4-2b51757f98d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reducedRDD.reduce(lambda x, y : (\"Total\", x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f14f838-fc7c-41ea-b89e-060800ad79b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we will perform a wordcount. In this example we will clean our data with the filter method to remove what we do not believe to be words and to make sure punctuation is not affecting our results.\n",
    "\n",
    "Notice how we can chain methods to create the cleaned RDD. We first apply a map transformation to convert all letters to lower case. We then apply a map transformation that removes all punctuation. Finally, we remove all empty strings. You do not need to be familiar with Python translate function that is removing the punctuation from our strings but this is a useful trick if you haven't seen it before.\n",
    "\n",
    "You have here an short tutorial on cleaning text data in Python in case you are interested: https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca363ceb-bdd3-4012-b843-f1327e180e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "#Create a flattened dataset split on the \" \" as a first aproximation of a split by word \n",
    "##(remember: spRDD is the original text file)\n",
    "\n",
    "#Clean up the dataset in the following steps: convert all words to lower case, remove all punctuation, remove all words with legnth 0\n",
    "## Tip: The maketrans() method returns a mapping table that can be used with the translate() method to replace specified characters.\n",
    "## Tip 2:  string.punctuation is a pre-initialized string used as string constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eabf8c78-0643-444d-9470-fc6fa9315eb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use the take command to take the first 20 elements of both of the new datasets (flatRDD, cleanedRDD). Notice the difference from the cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c4e66e-1a68-46d9-9df9-76c8f08ef090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec3320c-f970-4114-8bf0-5037c48c483d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090c02c3-3227-4daf-9a3e-70b83f322317",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we will convert our results to key value pairs again and reduce to find our most common words. Notice how first we convert to key value pairs, using map. Then we use reduceByKey, which is a reduce method that works with key value pairs. For reduceByKey we need to provide a reduction function with two inputs which represent the values in the key value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61a3717-06a8-42d8-aa17-d0f7cf18e10f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1bce865-bdf4-4817-9f44-4f2c2f4dbeff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a061cb0a-45fa-4c6e-98b2-3e7e0ee4c28b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use the filter and collect methods to find all words that occur more than 10000 times in the dataset. Remember the data set now consists of key-value pair tupples, and we want to filter only on the value part of the tupple.\n",
    "\n",
    "You results should look something like:\n",
    "\n",
    "Out[43]: [('or', 5745),\n",
    " ('what', 9137),\n",
    " ('one', 3727),\n",
    " ('live', 1064),\n",
    " ('yet', 3287),\n",
    " ('aside', 1356),\n",
    " ('on', 6461),\n",
    " ('his', 14500),\n",
    " ('heart', 2024),\n",
    " ('do', 7797),\n",
    " ('second', 1261),\n",
    " ('hath', 3993),\n",
    " ('may', 3644),\n",
    " ('exit', 2118),\n",
    " ('there', 3717)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273e7868-ccb9-4761-86c0-e461b283fcd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6d2090f-792b-4b7e-8938-39d50ee169f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use the sortBy to find the most common words, you will need to specify that you want to sort by the value part of the RDD (not the keys), and that you want to sort in descending order. An example of the syntax is below:\n",
    "\n",
    ".sortBy(***insert a lambda function here to show what you want to sort by***,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a948d498-654e-4b6d-b782-2a43de525b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "718a7653-e0fe-4fd5-a477-098a65c6ee4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Practice exercise\n",
    "\n",
    "In the below RDD I have loaded the shakespeare play Julius Caesar. Perform a wordcount to find the 10 most common words. You can insert more cells to work with if you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b22f8e1-56b7-4231-8d8d-0b5c83223c88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jcRDD = sc.textFile(\"file:/databricks/driver/julius-caesar_TXT_FolgerShakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb68ad8-59e9-4d01-b90d-2431ecb0cfa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#First we need to clean the dataset (split, lowercase, punctuation, and the lenght of the strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c939805d-f1c7-45da-b2ff-f0e5064c0484",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Then, do the count and sort by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f809eb36-fae1-41cd-9a2b-17a2516155d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Text information can be broadly categorized into two main types: facts and opinions. Facts\n",
    "are objective expressions about something. Opinions are usually subjective expressions that\n",
    "describe people’s sentiments, appraisals, and feelings toward a subject or topic.\n",
    "Sentiment analysis can be modeled as a classification problem:\n",
    "Classifying a sentence as expressing a positive, negative or neutral opinion, known as\n",
    "polarity classification.\n",
    "\n",
    "In an opinion, the entity the text talks about can be an object, its components, its aspects, its\n",
    "attributes, or its features. It could also be a product, a service, an individual, an organization,\n",
    "an event, or a topic. As an example, take a look at the opinion below:\n",
    "\n",
    "\"The battery life of this camera is too short.\"\n",
    "\n",
    "A negative opinion is expressed about a feature (battery life) of an entity (camera).\n",
    "A basic example of a rule-based implementation would be the following:\n",
    "Define two lists of polarized words (e.g. negative words such as bad, worst, ugly, etc and\n",
    "positive words such as good, best, beautiful, etc).\n",
    "Given a text:\n",
    "\n",
    "● Count the number of positive words that appear in the text.\n",
    "\n",
    "● Count the number of negative words that appear in the text.\n",
    "\n",
    "● If the number of positive word appearances is greater than the number of negative\n",
    "word appearances return a positive sentiment, conversely, return a negative\n",
    "sentiment. Otherwise, return neutral.\n",
    "\n",
    "This system is very naïve since it doesn't take into account how words are combined in a\n",
    "sequence or the strength of the positive or negative sentiment of the words. Instead we often\n",
    "use a dictionary with words scored on a scale of -5 to 5 with more negative values\n",
    "representing more negative words. We then look at the total or average score of all the\n",
    "words in a text to determine if it is positive or negative on average.\n",
    "\n",
    "Run the below cells to download a dictionary of words with their rating, higher ratings mean more positive words, negative rating means words with negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2517bd35-95cb-4b99-8526-d46e5cae81eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh wget https://gist.githubusercontent.com/damianesteban/06e8be3225f641100126/raw/a51c27d4e9cc242f829d895e23b4435021ab55e5/afinn-111.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ddacda-6c92-422d-8a3e-b0443e1dc256",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Tip: use split(\"\\t\") to split the text file using tab\n",
    "ratingRDD  = sc.textFile(\"file:/databricks/driver/afinn-111.txt\").map(lambda x: x.split(\"\\t\")).map(lambda x: (x[0], int(x[1])))\n",
    "ratingRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f00d2232-daab-4893-be47-29b2faaafb8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can see we have a list of words with different ratings in a pairedRDD. In case you are interested here is an online tool that lets you test the sentiment of a word according to this dictionary:\n",
    "\n",
    "https://darenr.github.io/afinn/\n",
    "\n",
    "We can now join our dictionary with the works of shakespare wordcount and count up the total negative and positive words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3231521-4b0d-4f6b-a0f2-d35ba564b3ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joinedRDD = reducedWordCountRDD.join(ratingRDD)\n",
    "joinedRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bea13947-9743-458c-b8af-92e929af0c95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You should see we now have a list of words from Shakespeare that also exist in our rating dictionary, we also have the number of times they occur and their rating. We can then combine the rating and number of times the words occur, and add up all the values to see if the overall sentiment of shakespear is positive or negative. See if you can understand how we are doing this with map and reduce below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c472267-3d38-4295-9e76-63ba0d39b37e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wordSentimentRDD = joinedRDD.map(lambda x: (x[0], x[1][0] * x[1][1]))\n",
    "wordSentimentRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0471d382-d145-4dfe-b7be-9ea931326bdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wordSentimentRDD.reduce(lambda x, y: (\"Total Sentiment\", x[1]+ y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc75cf9-2b14-409b-a65c-64b82ec4f57a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The sum of total sentiment is postive, so accoring to our analysis the works of Shakespeare contain a positive sentiment in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22522e02-0e63-4d8d-b7b4-a2db674e3132",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Practice exercise\n",
    "\n",
    "Can you find if the total sentiment of the play \"Julius Caesar\" is positive or negative? Remember you have already performed a wordcount on this play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92eab7fa-eab7-4ebb-a8c4-16c45e4648ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#First we need to join the RDD with the rating RDD\n",
    "\n",
    "#Then multiply\n",
    "\n",
    "#Finally sum up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89877e97-576c-4844-8058-2e44dad38418",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bonus exercise\n",
    "### Collocations\n",
    "Co-occurrence (or collocation) analysis is a simple and popular method in digital and computational humanities for measuring associations between actors, entities, and concepts using large collections of texts. For a few examples of how co-occurrence analysis has been used recently in humanistic scholarship, check out these research articles from Literary and Linguistic Computing.\n",
    "\n",
    "Weingart, Scott, and Jeana Jorgensen. 2013. “Computational Analysis of the Body in European Fairy Tales.” Literary and Linguistic Computing 28 (3): 404–16. doi:10.1093/llc/fqs015.\n",
    "\n",
    "This study was a collaboration between a digital historian and a gender and folklore studies scholar. They asked whether European fairy tales construct and represent bodies differently according to gender. They used a collection of 233 fairy tales, and tagged passages based on whether they referred to men or women, and whether those characters were young or old. They then looked for co-occurrences of body terms (head, heart, hands, beard) and adjectives, and looked for clusters of co-occurring terms that correspond disproportionately to gender or age.\n",
    "\n",
    "Pumfrey, Stephen, Paul Rayson, and John Mariani. 2012. “Experiments in 17th Century English: Manual versus Automatic Conceptual History.” Literary and Linguistic Computing 27 (4): 395–408. doi:10.1093/llc/fqs017.\n",
    "\n",
    "These authors used their own concordance program to study changes over time in usage of the term \"experiment\" and \"experimental\", based on the co-occurrence of those terms with other scientific and religious terms.\n",
    "\n",
    "Kimura, Fuminori, Takahiko Osaki, Taro Tezuka, and Akira Maeda. 2013. “Visualization of Relationships among Historical Persons from Japanese Historical Documents.” Literary and Linguistic Computing 28 (2): 271–78. doi:10.1093/llc/fqs045.\n",
    "\n",
    "The Hōgen Rebellion (1156) in Japan was a roughly two-week conflict between factions of former Emperor Sutoku and Emperor Goshirakawa over a dispute about Imperial succession, and about the degree of influence of the aristocratic Fujiwara clan that had heavily ingratiated the Imperial family. This was seen as an important factor in the transition from Imperial to samurai-led governance in Japan. In this study, the authors asked whether they could use computational methods to infer associations among aristocrats or samurai belonging to each of the two factions supporting Emperor Sutoku and Emperor Goshirakawa. They used a set of diaries called the \"Hyohanki,\" written by an aristocrat named Nobunori Taira between 1112 and 1187, which is considered to be a valuable source of information about the Hōgen Rebellion and surrounding events. Given a list of 78 people, and a list of Japanese place-names, they looked for co-occurrences of those people and places in specific diary entries, and used those co-occurrences to infer latent relationships among people based on their spatial activities. The assumption here is that if two people are operating in the same places, at around the same times, then they are more likely to have interacted with each other.\n",
    "\n",
    "Here we will use spark to perform collocation analysis.\n",
    "### Spark practice task\n",
    "Here we will practive using spark for text analysis:\n",
    "\n",
    "Start with the RDD shakespearRDD, remember in this RDD each line from the text is a row. Note these rows are stored as unicode, it will be useful to use transformation to create a new RDD which converts each line into word pairs. For example the following line \"Hello, my name is Bob.\" should be converted to the following rows:\n",
    "\n",
    "\"hello_my\"\n",
    "\n",
    "\"my_name\"\n",
    "\n",
    "\"name_is\"\n",
    "\n",
    "\"is_bob\"\n",
    "\n",
    "These pairs of words are sometimes called \"bigrams\". Note how we have: removed punctuation, converted to lower case, seperated words with a ' _ ' to create a bigram.\n",
    "\n",
    "Hint: An easy way to do this is to define a new method in python, apply your method to the \"Hello, my name is Bob.\" sentence and check you get the correct result (as above). You can then use the map function to apply this to the shakepeare RDD.\n",
    "\n",
    "For example your function should work like this:\n",
    "\n",
    "convertToWordPairs('Hello, my name is Bob.')\n",
    "\n",
    "Out[68]: ['hello_my', 'my_name', 'name_is', 'is_bob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83b41c7-7ff4-4f25-8007-a5a0a60d31f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def convertToWordPairs(line):\n",
    "    pairs = []\n",
    "    # Clean the data by removing punctuation, transforming strings to lower case and then split out the words\n",
    "    words = str(line).translate(str.maketrans('', '', string.punctuation)).lower().split(\" \")\n",
    "    # For each word (except the last) add that word and the word that follows as a bigram to the pair list\n",
    "    for i in range(0, len(words) -1):\n",
    "        # clean out any \"words\" with zero length\n",
    "        if (len(words[i]) > 0 and len(words[i+1]) > 0 ):\n",
    "            pairs.append('{}_{}'.format(words[i], words[i+1]))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d21ebee-154d-43ed-878f-9b290227aff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#run this cell to check you get the right output. The four bigrams shown above.\n",
    "convertToWordPairs('Hello, my name is Bob.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ead32d94-dff0-4a58-a87a-1daa31c707f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next you will want to apply your function to the text in such a way that we can create a RDD of key value pairs, where the Key is the bigram and the value is 1. We can these use the reduceByKey function to count the bigrams and create an RDD with bigram as the key and total count as the value.\n",
    "\n",
    "Your final output should look something like this (these results are unsorted):\n",
    "\n",
    "Out[72]: [('a_midsummer', 2),\n",
    " ('queen_of', 96),\n",
    " ('snug_joiner', 1),\n",
    " ('other_fairies', 3),\n",
    " ('youth_to', 15),\n",
    " ('hath_my', 49),\n",
    " ('rings_gauds', 1),\n",
    " ('within_his', 22),\n",
    " ('kind_wanting', 2),\n",
    " ('well_your', 13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d71dd747-0b18-4605-b027-cbb4bd5c43f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#First create de words pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f1a760-c149-4504-9fea-a00290fad6f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#And count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91af75bd-90d8-44d6-abf2-fe8e343bba0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Find the 10 most common bigrams.\n",
    "\n",
    "One option here would to be use the sortByKey function, however, this sorts by keys not values so you would need to figure out how to use it to sort by values. Alternatively there is a sort by function.\n",
    "\n",
    "The most common should be \"i_am\" with 1830 occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec63cc58-082b-4148-8a2e-0c30757e09c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 580384697165970,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lab5_bda_spark_class",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
