{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0cbb8e0-28af-4682-ab51-db0204595434",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Advanced Graph Analysis & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4383c448-391b-44da-ae17-8df5e96449f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this notebook, we will be using a combination of Natural Language Processing and network analysis to look at a Mafia network. The settings the mafia network is the following: As members of an investigation unit, we have been observing a network of families, which have been associated with nefarious activities. We will use two datasets:\n",
    "\n",
    "1. PDF files written by an undercover agent\n",
    "2. A network of interactions between those families\n",
    "\n",
    "For NLP in Spark we can use the open-source *spark-nlp* library, which allows us to use a variety of Deep Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e96512-c69e-4639-b6d0-a3e64990b51c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Since here we are given several PDFs to work with, we need a Python library to parse them using ``UDF``. We first need to install the external Python library ``pypdf``. The *graphframes* library, which we used in the previous lab, offers the useful ``GraphFrame``, but choices for graph algorithms are relatively limited. Thus, we will install the ``networkx`` library, which offers a range of popular graph algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada1760e-c4ee-4aaf-b287-fbe2bafdc9f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install pypdf networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0223681d-d05c-4368-99b9-da0850e97464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pypdf import PdfReader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bdd652c-baf9-4b04-b107-2b66a791d795",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We install the ``spark-nlp`` dependencies next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82df891a-f325-4f94-a3d0-8b81cd4eb821",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "# Start Spark Session\n",
    "spark = sparknlp.start()\n",
    "\n",
    "from sparknlp.base import DocumentAssembler, Pipeline, LightPipeline\n",
    "from sparknlp.annotator import (\n",
    "    Tokenizer,\n",
    "    WordEmbeddingsModel,\n",
    "    NerDLModel,\n",
    "    NerConverter\n",
    ")\n",
    "\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f92550-77ed-4b30-85db-7a96440e62f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We load all the reports from our agent as PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ec6615-3478-4899-8353-4318764ea0e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mafia_network_communications = spark.read.format(\"binaryFile\").load(\"dbfs:/FileStore/crime_letters/*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b101ccd-789c-462d-af3f-8e9eea3a0b34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mafia_network_communications.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc887e27-800a-452d-b770-0d4d72f6fbdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As the next step, we define a Python UDF that takes the binary content of each PDF and convert it to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2610bbe-5e3f-4fd6-bdea-a5cd0dc059d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def pdf_to_text(pdf) -> str:\n",
    "    \"\"\"\n",
    "    We transform a PDF (binary) into a string. The \"content\" column is already binary, so we read the bytes directly.\n",
    "    \"\"\"\n",
    "\n",
    "    # First we load the binary content\n",
    "    bytes_stream = io.BytesIO(pdf)\n",
    "\n",
    "    # We initialize the reader\n",
    "    reader = PdfReader(bytes_stream)\n",
    "\n",
    "    # As the final step, we go over each page (note though that in our case our PDFs have only one page each) and extract the text\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90050d3-873a-452a-9761-699200692b00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mafia_network_communications = mafia_network_communications \\\n",
    "  .withColumn(\"report_text\", pdf_to_text(F.col(\"content\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ae0643-216d-438d-b48f-cfec8c8a2080",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mafia_network_communications.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b75ce2-228a-4456-b13e-94140d651728",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Named Entity Recognition\n",
    "Named Entity Recognition (NER) is a natural language processing (NLP) technique that identifies and categorizes named entities within text into predefined categories such as names of persons, organizations, locations and dates. Our informant, who has infiltrated the organization, is sending regular letters. You are tasked with building a prototype of automatically processing the reports and extracting the names of the people in each letter, which can then be related to the larger network. \n",
    "\n",
    "We will define manually a pipeline that transforms our report text first into an embedding representation and then extracts our entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4d6bd0-2576-45c9-aaec-02e7342c2d39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Transforms raw texts to \"document\" annotation\n",
    "# Code here\n",
    "\n",
    "# Step 2: Tokenization\n",
    "# Code here\n",
    "\n",
    "# Step 3: Get the embeddings using glove_100d\n",
    "# Code here\n",
    "\n",
    "# Step 4: Use the ``ner_dl`` model\n",
    "# Code here\n",
    "\n",
    "# Step 5: Convert to NER\n",
    "# Code here\n",
    "\n",
    "# Define the pipeline\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88aa67c-3ecd-45b1-8e19-aa7254fdec8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "``Pipelines`` are a Spark concept that we will revisit during our labs on Machine Learning. You will find the API similar to that of ``scikit-learn`` in that it follows the fit/transform structure. Using pipelines, you may specify the steps of a sequence. Most often, you will find yourself using them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498e403c-aaeb-4d59-906e-95015214cef7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We fit our model \n",
    "# Code here\n",
    "\n",
    "# And transform the data\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732c02a1-9327-4937-8cc9-1b0d95d7cbb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Code here ner_results..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5d9f29-e8d0-471a-8bfb-1f0a71625f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let us now inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5273d07a-6ceb-414e-9995-50f5fd937115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35aff8ca-64e9-4f63-a9f4-4e39382c273d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To facilitate our analysis, we will use the ``path`` column to extract the date of the report and use the date to assign a report id. We will accomplish this by using a Window function. From there, we explode the array of struct (the result of the NER) and retrieve only the entity and the associated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f62762-d7eb-40db-a33c-67c894ea0310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c2170c-fc0c-494b-b1d7-79ee3e19e07c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. We extract the date\n",
    "# 2. Get a row ID based on date\n",
    "# 3. Explode the column\n",
    "# 4. Extract the results (in a struct)\n",
    "\n",
    "reports_parsed = ner_results \\\n",
    "    .withColumn(\"date\", F.to_date(F.regexp_extract(F.col(\"path\"), r\"(\\d{4}-\\d+-\\d{2})\", 1))) \\\n",
    "    .withColumn(\"report_id\", F.row_number().over(Window.orderBy(\"date\"))) \\\n",
    "    .withColumn(\"ner_exploded\", F.explode(\"ner\")) \\\n",
    "    .withColumns({\n",
    "        \"result\":  F.col(\"ner_exploded.result\"), \n",
    "        \"metadata\": F.col(\"ner_exploded.metadata.word\") \n",
    "    }\n",
    "    ) \\\n",
    "    .withColumn(\"row_number\", F.row_number().over(Window.orderBy(\"report_id\"))) \\\n",
    "    .select(F.col(\"result\"), F.col(\"metadata\"), F.col(\"report_id\"), F.col(\"row_number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a483f4f3-d642-4e64-b78f-93b1a033e08b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reports_parsed.limit(50).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64615088-9a40-4aa9-8f2c-45ef630be805",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We know that our agent always spells out the full name of the persons he follows (how convenient!). This allows us to do a clever join to get the people's full name: We join the DataFrame onto itself on the newly created variable ``row_number``, where the left side corresponds to the first name and the right side to the last name. Since we know the ordering we have as the join key (``row_number``, ``row_number - 1``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9109f9-9333-42e4-abbd-c1d387a0fad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sub_network = reports_parsed.alias(\"df1\")...\n",
    "\n",
    "#display(sub_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f953b61-81ec-485c-8137-b07add0477b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We have now extracted a subnet of the overall Mafia network. Our tasks are now two-fold:\n",
    "\n",
    "1. Which is the individual with the highest influence within the sub-network (based on each relation type)?\n",
    "2. Which is the individual with the highest influence within the overall network (based on each relation type)?\n",
    "\n",
    "Both of these question can be answered using network analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad213ed-289c-4828-b57e-dafdc406ee05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from graphframes import * \n",
    "\n",
    "nodes = spark.read.csv(\"dbfs:/FileStore/mafia_nodes.csv\", header=True)\n",
    "edges = spark.read.csv(\"dbfs:/FileStore/mafia_edges.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12125218-144e-4d57-931e-955c48cdce7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will create an index column for the relation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d09a5a19-3f66-464a-8eb7-23db3b3d6612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as tp\n",
    "\n",
    "edges = edges \\\n",
    "    .withColumn(\"relation_type_index\", F.dense_rank().over(Window.orderBy(\"relation_type\"))) \\\n",
    "    .withColumn(\"weight\", F.col(\"weight\").cast(tp.IntegerType()))\n",
    "edges.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6470cc-3201-4ccc-98a9-df9a2a90ca95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we instantiate our complete graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c02a467-8881-4774-9a96-5af11d1e3555",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mafia_graph = GraphFrame(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3a76425-b27f-42fa-969c-8a4f6c6211a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let us inspect the graph\n",
    "mafia_graph.vertices.show()\n",
    "mafia_graph.edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7641548a-1cb2-4b44-8f3a-04e697b8cff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "mafia_graph.edges.select(F.col(\"relation_type\")).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9757bf-1618-4208-a233-e5332e2d4a3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now subset our overall ``nodes`` DataFrame to extract the sub-network only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a09fa3-19fa-4774-a412-d75cf1591530",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sub_network_nodes = mafia_graph.vertices \\\n",
    "    .join(sub_network, on=[\"First Name\", \"Last Name\"], how=\"inner\") \\\n",
    "    .dropDuplicates([\"First Name\", \"Last Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994515fa-7cf9-4ebf-9ef7-c7517c159183",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mafia_graph_edges_sub_network = mafia_graph.edges...\n",
    "mafia_graph_edges_sub_network.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b2be719-6c79-446b-85e1-25c928b71787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get unique edges from subnetwork\n",
    "# Code here \n",
    "\n",
    "# Filter network by subnetwork nodes\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa165a08-ad59-48c4-b989-29c80a2ac3fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mafia_subgraph = GraphFrame(sub_network_nodes, sub_network_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bcbe0b0-471c-446b-b60e-6f143f49a537",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "While ``graphframes`` offers a method to compute ``degree centrality``, its inventory is relatively limited. Since we are operating on different partitions of the graph (i.e. the subgraph induced by the mentioned people in the reports and the entire graph), we can use Spark's capabilities and parallelize the operations. To this end, we will make use of the ``networkx`` library, which offers a wealth of functions to work with graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6be89e5-d523-4479-b80e-6b596b0d0610",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see, there are four different types of edges:\n",
    "- Asked for Meeting\n",
    "- Threatened\n",
    "- Sent Money\n",
    "- Called\n",
    "\n",
    "Those edge types give us the idea that this graph is directed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d6bdca-b969-45fa-a3c1-d5681dcbe965",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let us now proceed to our actual network analysis. We will compute to network centrality measures here, (in-/out-)degree centrality and betweenness centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f640e65e-7543-4393-b809-de1ee74cf77f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- *Degree centrality* measures the importance of a node in a network based on its connections. In the context of in-degree centrality, this metric quantifies how many incoming connections a node has, reflecting its popularity or influence within the network. Conversely, out-degree centrality assesses the number of outgoing connections from a node, indicating its capacity to disseminate information or influence others. \n",
    "\n",
    "- *Betweenness centrality*, on the other hand, evaluates the extent to which a node serves as a bridge or intermediary between other nodes in the network. Nodes with high betweenness centrality often lie on many shortest paths between pairs of nodes, suggesting their critical role in maintaining connectivity and facilitating communication within the network.\n",
    "\n",
    "Both of these, among many others, are implement in ``networkx``. To make us of these functionalities, we use a trick we learned in a previous class: ``pandas`` UDFs, which allow us to pass our graphframe or dataframe into a function and operate on it as normal Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53675d45-2475-43bf-bd06-df6f5fde7bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_schema_degree = tp.StructType([\n",
    "    tp.StructField(\"relation_type\", tp.StringType(), False),\n",
    "    tp.StructField(\"node\", tp.StringType(), False),\n",
    "    tp.StructField(\"in_degree_centrality\", tp.FloatType(), False),\n",
    "    tp.StructField(\"out_degree_centrality\", tp.FloatType(), False),\n",
    "])\n",
    "\n",
    "output_schema_betweenness = tp.StructType([\n",
    "    tp.StructField(\"relation_type\", tp.StringType(), False),\n",
    "    tp.StructField(\"node\", tp.StringType(), False),\n",
    "    tp.StructField(\"betweenness_centrality\", tp.FloatType(), False),\n",
    "])\n",
    "\n",
    "def nx_degree_centrality(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # We get the relation_type key\n",
    "    key = pdf[\"relation_type\"].iloc[0]\n",
    "\n",
    "    # Here we instantiate a networkx directed graph (DiGraph)\n",
    "    in_degree_centralities = nx.in_degree_centrality(nx.DiGraph(nx.from_pandas_edgelist(pdf, \"src\", \"dst\", edge_attr=\"weight\")))\n",
    "    out_degree_centralities = nx.out_degree_centrality(nx.DiGraph(nx.from_pandas_edgelist(pdf, \"src\", \"dst\", edge_attr=\"weight\")))\n",
    "    \n",
    "    # Finally, we return a pd.DataFrame\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"relation_type\": [key for _ in range(1, len(in_degree_centralities.values()) + 1)], \n",
    "            \"node\": in_degree_centralities.keys(), \n",
    "            \"in_degree_centrality\": in_degree_centralities.values(),\n",
    "            \"out_degree_centrality\": out_degree_centralities.values(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "def nx_betweenness_centrality(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # We get the relation_type key\n",
    "    key = pdf[\"relation_type\"].iloc[0]\n",
    "\n",
    "    # Here we instantiate a networkx directed graph (DiGraph)\n",
    "    betweenness_centrality = nx.betweenness_centrality(nx.DiGraph(nx.from_pandas_edgelist(pdf, \"src\", \"dst\", edge_attr=\"weight\")))\n",
    "    \n",
    "    # Finally, we return a pd.DataFrame\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"relation_type\": [key for _ in range(1, len(betweenness_centrality.values()) + 1)], \n",
    "            \"node\": betweenness_centrality.keys(), \n",
    "            \"betweenness_centrality\": betweenness_centrality.values()\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8af4bcf-a452-4cc3-ae5a-ff7d3cdacff3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We compute degree centrality and betweenness centrality for the subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae090ad-754d-491e-a7f2-1534c382d911",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summarize results for degree centrality\n",
    "mafia_subgraph \\\n",
    "    .edges \\\n",
    "    .groupby(\"relation_type\") \\\n",
    "    .applyInPandas(nx_degree_centrality, output_schema_degree) \\\n",
    "    .alias(\"degree_df\") \\\n",
    "    .join(mafia_graph.vertices.alias(\"node_df\"), F.col(\"degree_df.node\") == F.col(\"node_df.id\")) \\\n",
    "    .orderBy(\"relation_type\", \"in_degree_centrality\", ascending=False) \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601b8169-45f1-41d6-be99-64907e9f642a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summarize results for betweenness centrality\n",
    "mafia_subgraph \\\n",
    "    .edges \\\n",
    "    .groupby(\"relation_type\") \\\n",
    "    .applyInPandas(nx_betweenness_centrality, output_schema_betweenness) \\\n",
    "    .alias(\"degree_df\") \\\n",
    "    .join(mafia_graph.vertices.alias(\"node_df\"), F.col(\"degree_df.node\") == F.col(\"node_df.id\")) \\\n",
    "    .orderBy(\"relation_type\", \"betweenness_centrality\", ascending=False) \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ee8809-45d8-43a8-b6a2-d9754a424b68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We repeat this exercise for the entire graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97adcd54-7fb1-4888-8634-f7b450e02139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summarize results for degree centrality\n",
    "mafia_graph \\\n",
    "    .edges \\\n",
    "    .groupby(\"relation_type\") \\\n",
    "    .applyInPandas(nx_degree_centrality, output_schema_degree) \\\n",
    "    .alias(\"degree_df\") \\\n",
    "    .join(mafia_graph.vertices.alias(\"node_df\"), F.col(\"degree_df.node\") == F.col(\"node_df.id\")) \\\n",
    "    .orderBy(\"relation_type\", \"in_degree_centrality\", ascending=False) \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc1e73f2-c14a-40ac-8d99-149c2c112bfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summarize results for betweenness centrality\n",
    "mafia_graph \\\n",
    "    .edges \\\n",
    "    .groupby(\"relation_type\") \\\n",
    "    .applyInPandas(nx_betweenness_centrality, output_schema_betweenness) \\\n",
    "    .alias(\"degree_df\") \\\n",
    "    .join(mafia_graph.vertices.alias(\"node_df\"), F.col(\"degree_df.node\") == F.col(\"node_df.id\")) \\\n",
    "    .orderBy(\"relation_type\", \"betweenness_centrality\", ascending=False) \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac36b91-63e5-4944-b061-542be9b095ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Bonus\n",
    "If would like to compute the overall influence, without regard to the relation type, you can use a *fictional* relation type by creating a column with a literal value, such as 1 or a string. Note that using networkx does not leverage the speed of Spark, unless we partition our network in some way. Since we have a network with weights, we can use a groupby to sum the weight, which automatically imposes a uniqueness condition. We still need our fictional groupby to use ``applyInPandas``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ce9ecb-57a2-417a-8201-33698cda0ea7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mafia_graph \\\n",
    "    .edges \\\n",
    "    .withColumn(\"relation_type\", F.lit(\"1\")) \\\n",
    "    .groupby([\"src\", \"dst\", \"relation_type\"]) \\\n",
    "    .agg(F.sum(\"weight\").alias(\"weight\")) \\\n",
    "    .groupby(\"relation_type\") \\\n",
    "    .applyInPandas(nx_degree_centrality, output_schema_degree) \\\n",
    "    .alias(\"degree_df\") \\\n",
    "    .join(mafia_graph.vertices.alias(\"node_df\"), F.col(\"degree_df.node\") == F.col(\"node_df.id\")) \\\n",
    "    .orderBy(\"relation_type\", \"in_degree_centrality\", ascending=False) \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39cbe9e-2891-4dfe-9272-f24fce6f3e5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3646305723147179,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lab10_bda_class",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
