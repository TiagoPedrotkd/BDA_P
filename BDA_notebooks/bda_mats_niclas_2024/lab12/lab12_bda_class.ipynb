{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f958cfb1-077d-4c50-831f-554ec7fbbc46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ML Pipelines\n",
    "\n",
    "We are going to focus on preparing a data set by cleaning the data, creating new features, which are fields that will serve in training the model later, and then looking at selecting a curated set of features based on how promising they look.\n",
    "\n",
    "Lab based on book: Data Analysis with Python and PySpark, Jonathan Rioux\n",
    "\n",
    "‚ö†: This is not a class about in machine learning! For more about ML, look at Real-World Machine Learning by Henrik Brink, Joseph W. Richards, and Mark Fetherolf (Manning, 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974a171c-7456-4b6b-9f9b-549939fe6878",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Reading, exploring, and preparing our machine learning data set\n",
    "\n",
    "We will start with the ingestion and exploration of our machine learning data set. More specifically, we‚Äôll review the content of our data frame, look at incoherences, and prepare our data for feature engineering. For our ML model, I chose a data set of 20,057 dish names that contain 680 columns characterizing the ingredient list, the nutritional content, and the category of the dish. \n",
    "\n",
    "Dataset source: https://www.kaggle.com/datasets/hugodarwood/epirecipes\n",
    "\n",
    "üëç **Our goal here is to predict if this dish is a dessert**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7b13b3-b9e7-41ef-b51a-70405dc7556b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Import and clean\n",
    "\n",
    "Import the dataset into the cluster:\n",
    "\n",
    "- Click Data Icon Data in the sidebar.\n",
    "- Click the DBFS button at the top of the page.\n",
    "- Click the Upload button at the top of the page.\n",
    "- On the Upload Data to DBFS dialog, optionally select a target directory or enter a new one.\n",
    "- In the Files box, drag and drop or use the file browser to select the local file to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2620361c-7be1-4fad-b99c-bef195636c73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95718b4e-e0f1-4e71-8bb1-ee28b7b1bfd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/epi_r.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "food = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2b88ca-7caa-4458-8aa1-0f9491339aa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(food.count(), len(food.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b974c2-791e-41bf-886a-fcf86d6a347f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "food.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3fdd03-614c-4c3f-92dc-10800ff09d22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Some of the columns contains undesirable characters, such as a # (`#cakeweek`), or a space (`30 days of groceries`), or some invalid characters (`bon appÔøΩÔøΩtit`)!\n",
    "\n",
    "**Having a consistent column naming scheme will make subsequent code easier to write, read, and maintain in the long run.**\n",
    "\n",
    "We will remove anything that isn‚Äôt a letter or a number, standardize the spaces and other separators to use the underscore (_) character, andreplace the ampersand (&) with its English equivalent and.\n",
    "\n",
    "To apply our function `sanitize_column_name`, we used `toDF()`: when used to rename the colum ns of a data frame, takes as parameters N strings, where N is the number of columns in our data frame. Since we can access the columns of our data frame via `food.columns`, a quick list comprehension takes care of renaming everything. We also unpack my list into distinct attributes using the star operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ddfab2-07ca-4b10-8e16-67508a34d355",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_column_name(name):\n",
    "    \n",
    "    \"\"\"Drops unwanted characters from the column name.\n",
    "    We replace spaces, dashes and slashes with underscore, and only keep alphanumeric characters.\"\"\"\n",
    "    \n",
    "    answer = name\n",
    "    \n",
    "    for i, j in ((\" \", \"_\"), (\"-\", \"_\"), (\"/\", \"_\"), (\"&\", \"and\")):\n",
    "        answer = answer.replace(i, j)\n",
    "    return \"\".join(\n",
    "\n",
    "        [\n",
    "            char\n",
    "            for char in answer\n",
    "        if char.isalpha() or char.isdigit() or char == \"_\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    \n",
    "food = food.toDF(*[sanitize_column_name(name) for name in food.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6683f8f2-5df0-478b-b8b8-d926602770d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "food.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb50c0f1-4377-4c63-ab78-fe6d795cc2b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Explore and create features\n",
    "\n",
    "Exploring data for machine learning is similar to exploring data when performing a transformation in the sense that we manipulate the data to uncover some inconsistencies, patterns, or gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e38a2c2-37a2-40cc-882c-2da262754857",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8aaedb6-beb2-464f-8cf4-6b575051933c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Identifying your variables as categorical (with the proper subtype) or continuous has a\n",
    "direct impact on the data preparation and, down the road, the performance of your ML\n",
    "model. Looking at our summary data, it seems that we have a lot of potentially binary columns.\n",
    "In the case of the clove column, the minimum and three quartile values are all\n",
    "zero. To verify this, we‚Äôll group the entire data frame and collect a set of distinct values.\n",
    "If we have only two values for a given column, binary it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f873c68b-4440-48a0-8a8d-f651bbc9e275",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Is this colunm binary?\n",
    "#This CMD may take up to 1 min to run.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "\n",
    "is_binary = food.agg(\n",
    "    *[\n",
    "        (F.size(F.collect_set(x)) == 2).alias(x) \n",
    "        #collect_set() will create a set of the distinct values as an array, and size() returns the length of the array. Two distinct values means that it‚Äôs probably binary.\n",
    "        for x in food.columns\n",
    "    ]\n",
    ").toPandas()\n",
    "\n",
    "is_binary.unstack()\n",
    "#unstack un-pivots a pandas DataFrame, making a wide data frame easier to analyze in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f765e4d5-47ac-4ddd-85b2-fa99f381ecb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Data mishapes and feature set\n",
    "\n",
    "Some columns are not\n",
    "consistent compared to other related (binary) columns. We are going to explore the content of\n",
    "the suspicious columns, address the gaps, and continue our exploration. We aim a\n",
    "more consistent, more robust feature set that will lead to a better ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5a3a55-e589-4220-985a-9ba36113eb01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "food.agg(*[F.collect_set(x) for x in (\"cakeweek\", \"wasteless\")]).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c46655-f566-463a-b786-c3184b1578ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "food.where(\"cakeweek > 1.0 or wasteless > 1.0\").select(\"title\", \"rating\", \"wasteless\", \"cakeweek\", food.columns[-1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a3dabd-201e-45af-9631-d68b88098ef0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our data set had a bunch of quotation marks along with some commas that confused PySpark‚Äôs parser. Since\n",
    "we have a small number of records affected, I did not bother with realigning the data\n",
    "and deleted them outright. I keep the null values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f25e79a-dde6-47ee-9c59-31c02196af52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "food = food.where(\n",
    "    (\n",
    "        #\"if cakeweek and wasteless are both either 0.0, 1.0, or null.\"\n",
    "        F.col(\"cakeweek\").isin([0.0, 1.0])\n",
    "        | F.col(\"cakeweek\").isNull()\n",
    "    )\n",
    "    & (\n",
    "        F.col(\"wasteless\").isin([0.0, 1.0])\n",
    "        | F.col(\"wasteless\").isNull()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b50c63f-4df7-46a9-8b26-62e8dc2d5de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#we expect 3 less records:\n",
    "print(food.count(), len(food.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174e4088-501d-4433-821a-f9d5fe1f00a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we have identified two binary-in-hiding feature columns, we can identify our feature set and our target variable. The target (or label) is the column containing\n",
    "the value we want to predict. In our case, the column is aptly named `dessert`.\n",
    "\n",
    "Let's create all-caps variables containing the four main sets of columns I\n",
    "care about:\n",
    "- The identifiers, which are the column(s) that contain the information unique to\n",
    "each record\n",
    "- The targets, which are the column(s) (most often one) that contain the value we\n",
    "wish to predict\n",
    "- The continuous columns, containing continuous features\n",
    "- The binary columns, containing binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4785a649-2149-4ca3-a54b-d251b20c8d82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "IDENTIFIERS = [\"title\"]\n",
    "\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    \"rating\",\n",
    "    \"calories\",\n",
    "    \"protein\",\n",
    "    \"fat\",\n",
    "    \"sodium\",\n",
    "]\n",
    "\n",
    "TARGET_COLUMN = [\"dessert\"]\n",
    "\n",
    "BINARY_COLUMNS = [\n",
    "    x\n",
    "    for x in food.columns\n",
    "    if x not in CONTINUOUS_COLUMNS\n",
    "    and x not in TARGET_COLUMN\n",
    "    and x not in IDENTIFIERS\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c981bef4-5190-4db6-82db-607922fefe6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.4 Find and delete useless records and input binary features\n",
    "\n",
    "I will removing two types of records:\n",
    "- Those where all the features are null\n",
    "- Those where the target is null\n",
    "\n",
    "Furthermore, we will impute, meaning that we will provide a default value for, our\n",
    "binary features. Since each of them are 0/1, where zero is False and one is True, we\n",
    "equate null to False and fill zero as a default value (not ideal, but reasonable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2777230-50e1-4cb7-88da-515342a043be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#FIRST: remove records with only null values\n",
    "\n",
    "food = food.dropna(\n",
    "    how=\"all\",\n",
    "    subset=[x for x in food.columns if x not in IDENTIFIERS],\n",
    ")\n",
    "\n",
    "food = food.dropna(subset=TARGET_COLUMN)\n",
    "\n",
    "print(food.count(), len(food.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad4dfbdf-74e8-4b36-ae8f-7b5843c50998",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "(we lost 5 records, thats ok!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339e04e2-e8b4-4b82-a5e1-122f6367930d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#SECOND: impute a default value (0.0) to all binary columns\n",
    "\n",
    "food = food.fillna(0.0, subset=BINARY_COLUMNS)\n",
    "\n",
    "print(food.where(F.col(BINARY_COLUMNS[0]).isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f299b2e1-15d9-41d6-a3b0-756907dc47a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.5 Cleaning continuous variables (and extreme values)\n",
    "\n",
    "We are going to:\n",
    "- cast the variables and delete wrong values\n",
    "- review the distribution of numerical columns to account for\n",
    "extreme or unrealistic values.\n",
    "\n",
    "‚ö† The next steps are not a blueprint to be applied regardless\n",
    "of the situation/dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58dc1f61-1cd6-423c-8c0f-05839fa0ab71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If we go back to the schema in section 1.1, because of some data misalignment, PySpark\n",
    "inferred the type of the rating and calories column as a string, where they should\n",
    "have been numerical.\n",
    "\n",
    "We are going to create an UDF (user defined function) to take a string column and return\n",
    "True if the value is a floating-point number (or a null‚ÄîPySpark will allow null values\n",
    "in a Double column) and False otherwise.\n",
    "\n",
    "More about UDF: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "\n",
    "The function returns True right off the bat if the value is null. If the value is a non-null value, it casts the value as a Python float. If it fails, it returns False.\n",
    "\n",
    "üëçTIP: If you want to negate a whole expression in a filter() method, PySpark provides the ~ operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac1de639-aa90-4ec8-94b0-551c8560e1a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#FIRST: cast\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "@F.udf(T.BooleanType())\n",
    "def is_a_number(value: Optional[str]) -> bool:\n",
    "    if not value:\n",
    "        return True\n",
    "    try:\n",
    "        _ = float(value) #We used the underscore to tell the code to perform the work, but to not care about the result.\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "food.where(~is_a_number(F.col(\"rating\"))).select(*CONTINUOUS_COLUMNS).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d1ec7d-dad5-49c4-a40f-4b601630eba0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We have a single remaining rogue record that we remove in the next CMD before casting the columns as a double. Our continuous feature columns are now all numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dfecda0-7b98-45d2-801c-ce84d43e6dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for column in [\"rating\", \"calories\"]:\n",
    "    food = food.where(is_a_number(F.col(column)))\n",
    "    food = food.withColumn(column, F.col(column).cast(T.DoubleType()))\n",
    "\n",
    "print(food.count(), len(food.columns)) #we should lose just one record!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333a9498-bde3-4d7f-b035-862ad14a5b11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We need to use our judgment for the best course of\n",
    "action to address this data quality issue. I could filter the records once more, but this\n",
    "time, I‚Äôll cap the values to the 99th percentile, avoiding extreme (and potentially\n",
    "wrong) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37e06cb-0647-4b26-8746-5f4d50444b07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#SECOND: Look for extreme values\n",
    "food.select(*CONTINUOUS_COLUMNS).summary(\n",
    "\"mean\",\n",
    "\"stddev\",\n",
    "\"min\",\n",
    "\"1%\",\n",
    "\"5%\",\n",
    "\"50%\",\n",
    "\"95%\",\n",
    "\"99%\",\n",
    "\"max\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7289f4-1dae-4072-8a85-2d59cbb4e8d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To make things easier, we are going to **hardcode** the maximum acceptable values for each column, and\n",
    "then I apply those maximums iteratively to my food data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f140b967-7720-4e7f-ac42-889c20d3ec2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "maximum = {\n",
    "    \"calories\": 3203.0,\n",
    "    \"protein\": 173.0,\n",
    "    \"fat\": 207.0,\n",
    "    \"sodium\": 5661.0,\n",
    "}\n",
    "\n",
    "for k, v in maximum.items():\n",
    "    food = food.withColumn(\n",
    "        k,\n",
    "        F.when(F.isnull(F.col(k)), F.col(k)).otherwise( \n",
    "            F.least(F.col(k), F.lit(v))\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef87cbd-9d47-49dd-b3d0-fef6115b3560",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "food.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e60d4506-7f39-44cd-8521-0a604f825f82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.6 Remove rare binary features\n",
    "\n",
    "We are going to remove features that are either too rare or too frequent. Binary features with only a few zeroes or ones are not\n",
    "helpful in classifying a recipe as a dessert: if every recipe (or no recipe) has a certain\n",
    "feature as true, then that feature does not discriminate properly, meaning that our\n",
    "model has no use for it.\n",
    "\n",
    "In last section, we computed the sum of each\n",
    "binary column; this will give us the numbers of 1.0's since the sum of the ones is equal to their count.\n",
    "\n",
    "For this model, let's use 10 as threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c488fdb-2136-4308-a940-8f1a439ca5d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inst_sum_of_binary_columns = [\n",
    "    F.sum(F.col(x)).alias(x) for x in BINARY_COLUMNS\n",
    "]\n",
    "\n",
    "sum_of_binary_columns = (\n",
    "    food.select(*inst_sum_of_binary_columns).head().asDict()  # Since a row is just like a Python dictionary, I can bring the row back to the driver and process it locally.\n",
    ")\n",
    "\n",
    "num_rows = food.count()\n",
    "too_rare_features = [\n",
    "    k\n",
    "    for k, v in sum_of_binary_columns.items()\n",
    "    if v < 10 or v > (num_rows - 10)\n",
    "]\n",
    "\n",
    "print('count of variables to remove: ', len(too_rare_features))\n",
    "\n",
    "print('\\n\\n\\nvariables to remove: \\n',too_rare_features)\n",
    "\n",
    "#Rather than deleting the columns from the data frame, I just remove them from my BINARY_COLUMNS list.\n",
    "BINARY_COLUMNS = list(set(BINARY_COLUMNS) - set(too_rare_features))\n",
    "\n",
    "print('\\n\\n\\n\\nvariable kept: \\n',BINARY_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a39919b-1627-401c-a7cb-b8ce151fbb34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "(We removed 167 features that are either too rare or too frequent.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8751b0e-8cb5-488d-b38e-dd6bc438071a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Now we are going into two important steps of model building: feature creation (also called\n",
    "feature engineering) and refinement.\n",
    "\n",
    "Our goal:\n",
    "- Creating a few custom features using our continuous feature columns\n",
    "- Measuring correlation over original and generated continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea642392-b500-441b-857b-59f35f5b6d96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Customs features\n",
    "\n",
    "In PySpark, creating\n",
    "new features is done simply by creating columns with the information you want;\n",
    "this means you can create simple or highly sophisticated features.\n",
    "\n",
    "Just as an example, we‚Äôll take the `protein` and `fat` columns representing\n",
    "the quantity (in grams) of protein and fat in the recipe, respectively. With the information\n",
    "in those two columns, I create two features representing the percentage of calories\n",
    "attributed to each macro nutriment.\n",
    "\n",
    "‚ö† PAY ATENTION TO MULTICOLLINEARITY! When\n",
    "using a model that has a linear component, such as the linear regression and the\n",
    "logistic regression, this will cause problems with your model‚Äôs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb92ff0c-b818-4202-8623-56e8255fcf1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "food = food.withColumn(\n",
    "    \"protein_ratio\", F.col(\"protein\") * 4 / F.col(\"calories\")  # <1>\n",
    ").withColumn(\n",
    "    \"fat_ratio\", F.col(\"fat\") * 9 / F.col(\"calories\")\n",
    ") #There are 4 kcal per grams of protein and 9 kcal per grams of fat.\n",
    "\n",
    "food = food.fillna(0.0, subset=[\"protein_ratio\", \"fat_ratio\"])\n",
    "\n",
    "CONTINUOUS_COLUMNS += [\"protein_ratio\", \"fat_ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de424b7-9916-445c-8110-c31ba044cbce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Feature correlation\n",
    "\n",
    "Look at the correlation\n",
    "between our set of continuous may help us improve our model accuracy and explainability\n",
    "\n",
    "In this section we are going to address:\n",
    "1. How PySpark computes the correlation between variables and provides the results in a matrix using Vector and Matrix objects\n",
    "2. How we can extract values from them. \n",
    "3. The correlation between our continuous variables and made a decision about their inclusion in our first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "177aa730-d0f4-45a5-983d-168d1d2e5c07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For computing correlation between variables, PySpark provides the `Correlation` object.\n",
    "Correlation has a single method, `corr`, that computes the correlation between features\n",
    "in a `Vector`. Vectors are like PySpark arrays but with a special representation optimized\n",
    "for ML work.\n",
    "We are going to use the `VectorAssembler` transformer on the food data frame to create a new column,\n",
    "continuous_features, that contains a Vector of all our continuous features.\n",
    "A transformer is a preconfigured object that, as its name indicates, transforms a\n",
    "data frame. Independently, it looks like unnecessary complexity, but it shines when\n",
    "applied within a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0692e4f3-5f4a-433f-84ab-fb0fbc0656db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "continuous_features = VectorAssembler(\n",
    "    inputCols=CONTINUOUS_COLUMNS, outputCol=\"continuous_features\"\n",
    ")\n",
    "\n",
    "vector_food = food.select(CONTINUOUS_COLUMNS)\n",
    "for x in CONTINUOUS_COLUMNS:\n",
    "    vector_food = vector_food.where(~F.isnull(F.col(x))) \n",
    "\n",
    "vector_variable = continuous_features.transform(vector_food)\n",
    "\n",
    "vector_variable.select(\"continuous_features\").show(3, False)\n",
    "\n",
    "#Correlation will not work well if you blend categorical and/or binary features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182a5b16-6b10-452d-888b-bbe5b8961460",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vector_variable.select(\"continuous_features\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621d1c41-a450-41d9-9723-a02be6626347",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now, we are going to apply the `Correlation.corr()` function on the continuous feature\n",
    "vector and export the correlation matrix into an easily interpretable pandas Data-\n",
    "Frame. PySpark returns the correlation matrix in a `DenseMatrix` column type, which\n",
    "is like a two-dimensional vector. In order to extract the values in an **easy-to-read format**:\n",
    "1. We extract a single record as a list of Row using head().\n",
    "2. A Row is like an ordered dictionary, so we can access the first (and only) field\n",
    "containing our correlation matrix using list slicing.\n",
    "3. A DenseMatrix can be converted into a pandas-compatible array by using the\n",
    "toArray() method on the matrix.\n",
    "4. We can directly create a pandas DataFrame from our Numpy array. Inputting\n",
    "our column names as an index (in this case, they‚Äôll play the role of ‚Äúrow names‚Äù)\n",
    "makes our correlation matrix very readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a443812-ed98-47f6-917a-b44a3e433ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "#The corr method takes a data frame and a Vector column reference as a parameter and generates a single-row, single column data frame containing the correlation matrix.\n",
    "correlation = Correlation.corr(\n",
    "    vector_variable, \"continuous_features\"\n",
    ")\n",
    "\n",
    "correlation.printSchema()\n",
    "\n",
    "#DenseMatrix is not easily accessible by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17280076-bed1-4bdc-84d7-9444d0677dee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "correlation_array = correlation.head()[0].toArray()\n",
    "\n",
    "correlation_pd = pd.DataFrame(\n",
    "    correlation_array,  \n",
    "    index=CONTINUOUS_COLUMNS,  \n",
    "    columns=CONTINUOUS_COLUMNS, \n",
    ")\n",
    "\n",
    "print(correlation_pd.iloc[:, :6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67369f96-db49-489e-9eb6-e8e8cdbf0fbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**There is no absolute threshold for keeping or removing correlated variables.**\n",
    "\n",
    "We see high correlation between `sodium`,\n",
    "`calories`, `protein`, and `fat`. Surprisingly, we see little correlation between our custom\n",
    "features and the columns that contributed to their creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00cedb5a-e3e8-4424-a088-e120f3d8ad84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Feature Preparation\n",
    "\n",
    "This section provides an overview of transformers and estimators in the context of feature\n",
    "preparation. We use transformers and estimators as an abstraction over common\n",
    "operations in machine learning modeling. We explore two relevant examples of transformers\n",
    "and estimators:\n",
    "-  Null imputation, where we provide a value to replace null occurrences in a column\n",
    "(e.g., the mean)\n",
    "-  Scaling features, where we normalize the values of a column, so they are on a\n",
    "more logical scale (e.g., between zero and one)\n",
    "\n",
    "\n",
    "The best way to think about a `transformer` is by translating its behavior into a\n",
    "`function`. Below we compare a `VectorAssembler` to a `function` assemble_\n",
    "vector() that performs the same work, which is to create a Vector named after the\n",
    "argument to outputCol, which contains all the values in the columns passed to\n",
    "inputCols. Don‚Äôt focus on the actual work here, but more on the mechanism\n",
    "of application.\n",
    "\n",
    "![image](files/tables/transformer.jpg)\n",
    "\n",
    "The transformer object has a two-staged process. \n",
    "- First, when instantiating the\n",
    "transformer, we provide the parameters necessary for its application, but not the data\n",
    "frame on which it‚Äôll be applied. This echoes the separation of data and instructions we\n",
    "saw in previously Labs. \n",
    "- Then, we use the instantiated transformer‚Äôs transform() method on\n",
    "the data frame to get a transformed data frame.\n",
    "\n",
    "\n",
    "This separation of instructions and data is key in creating serializable ML pipelines,\n",
    "which leads to easier ML experiments and model portability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007ed094-4749-4c8f-b2c6-2ee8fb7c05ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.1 Imputer estimator\n",
    "\n",
    "In this section, we cover the Imputer estimator and introduce the concept of an estimator.\n",
    "Estimators are the main abstraction used by Spark for any data-dependent transformation,\n",
    "including ML models, so they are pervasive in any ML code using PySpark.\n",
    "\n",
    "We want our Imputer to impute the mean value to every record in the\n",
    "calories, protein, fat, and sodium columns when the record is null.\n",
    "\n",
    "More information in section Imputer: https://spark.apache.org/docs/latest/ml-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ceda3d9-5a63-4b24-9b34-78084d14c355",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "OLD_COLS = [\"calories\", \"protein\", \"fat\", \"sodium\"]\n",
    "NEW_COLS = [\"calories_i\", \"protein_i\", \"fat_i\", \"sodium_i\"]\n",
    "\n",
    "imputer = Imputer(\n",
    "    strategy=\"mean\",  \n",
    "    inputCols=OLD_COLS,  \n",
    "    outputCols=NEW_COLS,  \n",
    ")\n",
    "\n",
    "imputer_model = imputer.fit(food)\n",
    "\n",
    "CONTINUOUS_COLUMNS = (\n",
    "    list(set(CONTINUOUS_COLUMNS) - set(OLD_COLS)) + NEW_COLS  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15a6018-9efe-4461-a8f0-3fca42204f63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Let's check!\n",
    "\n",
    "food_imputed = imputer_model.transform(food)\n",
    "\n",
    "food_imputed.where(\"calories is null\").select(\"calories\", \"calories_i\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b888a283-5853-4962-ae86-53237d39ef78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.2 Scaling features\n",
    "\n",
    "This section covers variable scaling using the MinMaxScaler transformer. Scaling variables\n",
    "means performing a mathematical transformation on the variables so that they\n",
    "are all on the same numeric scale.\n",
    "\n",
    "To choose the right scaling algorithm, we need to look at our variables as a whole.\n",
    "Since we have so many binary variables, it is convenient to have every variable be\n",
    "between zero and one. Our protein_ratio and fat_ratio are ratios between zero\n",
    "and one too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699077d1-2e6b-4559-a5bf-235b0b7ba45d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "CONTINUOUS_NB = [x for x in CONTINUOUS_COLUMNS if \"ratio\" not in x]\n",
    "\n",
    "continuous_assembler = VectorAssembler(\n",
    "    inputCols=CONTINUOUS_NB, outputCol=\"continuous\"\n",
    ")\n",
    "\n",
    "food_features = continuous_assembler.transform(food_imputed)\n",
    "\n",
    "continuous_scaler = MinMaxScaler(\n",
    "    inputCol=\"continuous\",\n",
    "    outputCol=\"continuous_scaled\",\n",
    ")\n",
    "\n",
    "food_features = continuous_scaler.fit(food_features).transform(\n",
    "    food_features\n",
    ")\n",
    "\n",
    "food_features.select(\"continuous_scaled\").show(3, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e39cb19-00d9-410f-b4b5-9464d1159a29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "üëç TIP check the pyspark.ml.feature module for other scalers. https://spark.apache.org/docs/2.3.1/api/python/pyspark.ml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7e37e1-589c-44f2-acdd-d808ed892beb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Finally, ML Pipeline\n",
    "\n",
    "We may say an ML pipeline is\n",
    "an ordered list of transformers and estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d1614d-063c-442c-80c5-bac1e8c6d94b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.1 Transformers and estimators\n",
    "\n",
    "\n",
    "TRANSFORMERS:\n",
    "\n",
    "Transformer‚Äôs sole purpose‚Äîthrough its `transform()` method‚Äîis to take\n",
    "the values in `inputCols` (assembled values) and return a single column, named\n",
    "`outputCol`, that contains a vector of all the assembled values.\n",
    "A transformer has a set of explicit parameters (called\n",
    "Params in the Spark language) that drive its behavior.\n",
    "Some parameters have a default value in case you\n",
    "don‚Äôt define a value yourself (e.g., handleInvalid).\n",
    "\n",
    "The most important method of a\n",
    "transformer is the `transform( )`\n",
    "method. This method takes a data\n",
    "frame as an input and returns\n",
    "a transformed data frame.\n",
    "\n",
    "Example: `VectorAssembler` is a transformer. Params: inputCols, outputCol, handleInvalid\n",
    "\n",
    "If you look at the signature for VectorAssembler, you‚Äôll see an asterisk at the beginning\n",
    "of the parameters list:\n",
    "\n",
    "`` class pyspark.ml.feature.VectorAssembler(*, inputCols=None,\n",
    "outputCol=None, handleInvalid='error')`` \n",
    "\n",
    "In Python, every parameter after the asterisk (*) is called a keyword-only argument,\n",
    "meaning that we need to mention the keyword. For instance, we couldn‚Äôt do Vector-\n",
    "Assembler(\"input_column\", \"output_column\"). For more: https://peps.python.org/pep-3102/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ceebc7-55c4-44b1-a7ac-e19fb18f5daf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(continuous_assembler.outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec57a3e2-18c0-4cd6-9a90-179f181978b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(continuous_assembler.getOutputCol())\n",
    "\n",
    "print('\\n', continuous_assembler.explainParam(\"outputCol\"))\n",
    "\n",
    "print('\\n', continuous_assembler.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e9bafa-c5c2-4ba0-beb0-08c5d39fe737",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "ESTIMATOR:\n",
    "\n",
    "Where a transformer transforms an\n",
    "input data frame into an output data frame, an estimator is fitted on an input data\n",
    "frame and returns an output transformer.\n",
    "\n",
    "We focus on estimator usage through the `fit()`\n",
    "method (versus `transform()` for the transformer), which is really the only notable\n",
    "difference for the end user. The `fit()` method takes a data\n",
    "frame as an input and returns a parametrized\n",
    "transformer as an output.\n",
    "\n",
    "Just like a transformer, an estimator has a set\n",
    "of explicit parameters (called Params in the\n",
    "Spark language) that drive its behavior. Some\n",
    "parameters have a default value in case you\n",
    "don‚Äôt define a value yourself (e.g., min/max).\n",
    "\n",
    "Example: `MinMaxScaler` is a estimator. Params: `min`, `max`, `inputcCol`, `outputCol`.\n",
    "\n",
    "This fit()/transform() approach applies for estimators that are far more complex\n",
    "than MinMaxScaler. Case in point: ML models are actually implemented as estimators\n",
    "in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be12cce-b09d-4f61-b729-e25741dcd78f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.2 Building a complete ML pipeline\n",
    "\n",
    "This section we will introduce the `Pipeline` object as an estimator with a special purpose:\n",
    "running other transformers and estimators.\n",
    "\n",
    "Pipelines build on transformers and estimators\n",
    "to make training, evaluating, and optimizing ML models much clearer and\n",
    "more explicit.\n",
    "\n",
    "ML pipelines are implemented through the Pipeline class, which\n",
    "is a specialized version of the estimator. The Pipeline estimator has only one\n",
    "Param, called stages, which takes a list of transformers and estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b220b5-6dc6-41b5-b73e-3b8c8ae7fc42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Just as a matter of completeness, we are going to repeat the estimators and transformers here, to consolidate the code \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.feature as MF\n",
    "\n",
    "imputer = MF.Imputer(  \n",
    "    strategy=\"mean\",\n",
    "    inputCols=[\"calories\", \"protein\", \"fat\", \"sodium\"],\n",
    "    outputCols=[\"calories_i\", \"protein_i\", \"fat_i\", \"sodium_i\"],\n",
    ")\n",
    "\n",
    "continuous_assembler = MF.VectorAssembler(  \n",
    "    inputCols=[\"rating\", \"calories_i\", \"protein_i\", \"fat_i\", \"sodium_i\"],\n",
    "    outputCol=\"continuous\",\n",
    ")\n",
    "\n",
    "continuous_scaler = MF.MinMaxScaler(  \n",
    "    inputCol=\"continuous\",\n",
    "    outputCol=\"continuous_scaled\",\n",
    ")\n",
    "\n",
    "\n",
    "#The food_pipeline pipeline contains three stages, encoded in the stages Param\n",
    "food_pipeline = Pipeline(  \n",
    "    stages=[imputer, continuous_assembler, continuous_scaler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b40911-b46f-48cf-87cc-de6402023f5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In practical terms, since the pipeline is an estimator, it has a `fit()` method that generates\n",
    "a PipelineModel. Under the hood, the pipeline applies each stage in order, calling\n",
    "the appropriate method depending on if the stage is a transformer (`transform()`)\n",
    "or an estimator (`fit()`). By wrapping all of our individual stages into a pipeline, we\n",
    "only have one method to call, `fit()`, knowing that PySpark will do the right thing to\n",
    "yield a PipelineModel.\n",
    "\n",
    "If the stage is\n",
    "a transformer, it gets applied on the data\n",
    "and then passed as a stage in the\n",
    "PipelineModel. If the stage is an estimator,\n",
    "it gets fitted on the data and the resulting\n",
    "model gets passed as a stage in the\n",
    "PipelineModel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80cec4a4-6629-47eb-96cc-8f993714cf5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 4.2.1 Final dataset (vector column type)\n",
    "\n",
    "This section will cover the assembly into a final feature vector, the last stage before\n",
    "sending our data for training.\n",
    "\n",
    "PySpark requires all the data fed into a machine learning\n",
    "estimator, as well as some other estimators like the MinMaxScaler, to be in a single vector\n",
    "column.\n",
    "\n",
    "REMEMBER: We already know how to assemble data into a vector: use the `VectorAssembler`.\n",
    "\n",
    "We will assemble all of our BINARY_COLUMNS, the _ratio columns, and the continuous_\n",
    "scaled vector column from our pipeline. PySpark will do the right thing when assembling\n",
    "vector columns in another vector: rather than getting nested vectors, the assembly\n",
    "step will flatten everything into a single, ready-to-use vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f77ddf-6569-4745-8803-88562d562b85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preml_assembler = MF.VectorAssembler(\n",
    "    inputCols=BINARY_COLUMNS \n",
    "    + [\"continuous_scaled\"]\n",
    "    + [\"protein_ratio\", \"fat_ratio\"],\n",
    "    outputCol=\"features\",\n",
    ")\n",
    "\n",
    "food_pipeline.setStages(\n",
    "    [imputer, continuous_assembler, continuous_scaler, preml_assembler]\n",
    ")\n",
    "\n",
    "food_pipeline_model = food_pipeline.fit(food)  # food_pipeline_model becomes a PipelineModel\n",
    "food_features = food_pipeline_model.transform(food) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de04576f-f0b6-4204-af3a-bf740ae47a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our data frame is ready for machine learning! We have a number of records, each with\n",
    "- A target (or label ) column, dessert, containing a binary input (1.0 if the recipe\n",
    "is a dessert, 0.0 otherwise)\n",
    "- A vector of features, called features, containing all the information we want to\n",
    "train our machine learning model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2762fc6a-9afa-45dd-a27f-0703cb3b966e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "food_features.select(\"title\", \"dessert\", \"features\").show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4acd4f0-a9eb-4259-81bf-802af335f231",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We provide 513 distinct features (see the 513 at the beginning of the features column value) with a large number of zeroes. This is\n",
    "called a sparse features set. When storing vectors, PySpark has two choices for representing\n",
    "vectors:\n",
    "-  A dense representation, where a Vector in PySpark is simply a NumPy (a highperformance\n",
    "multidimensional array library for Python) single-dimensional\n",
    "array object\n",
    "-  A sparse representation, where a Vector in PySpark is an optimized sparse vector\n",
    "compatible with the SciPy (a scientific computing library in Python)\n",
    "scipy.sparse matrix.\n",
    "\n",
    "For more: https://www.youtube.com/watch?v=oGwEv82ifrE\n",
    "\n",
    "\n",
    "PySpark allows for a metadata dictionary to be attached to a\n",
    "column, let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b09148d-ce6c-4193-b938-1e91dede9e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(food_features.schema[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02390197-9d14-49cd-8623-2963825c1461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(food_features.schema[\"features\"].metadata)\n",
    "\n",
    "# Since they originate from a VectorAssembler, PySpark gives scaled variables a generic name, but you can retrieve their name from the original vector column (here continuous_assembled) as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83fc124-9716-49b2-91ba-25380c40c1b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 4.2.2 Trainning the model (using a Logistics Regression)\n",
    "\n",
    "It is time do add a ML model to our Pipeline!\n",
    "\n",
    "ATTENTION: in real world, you need to choose the correct model to apply to the business problem\n",
    "you are trying to solve\n",
    "\n",
    "In our case, because our target is binary (0.0 or 1.0), we restrict ourselves to a classification algorithm. The logistic regression algorithm, despite its name, is a classification algorithm that\n",
    "belongs to the family of generalized linear models.\n",
    "\n",
    "Before integrating our logistic regression into our pipeline, we need to create the `estimator`.\n",
    "This `estimator` is called `LogisticRegression` and comes from the `pyspark.ml\n",
    ".classification` module. The API documentation page for the LogisticRegression: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2a9b7b-2680-47cb-ab77-b024ab4cc703",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"dessert\", predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "food_pipeline.setStages(\n",
    "    [\n",
    "        imputer,\n",
    "        continuous_assembler,\n",
    "        continuous_scaler,\n",
    "        preml_assembler,\n",
    "        lr,  # <1>\n",
    "    ]\n",
    ")\n",
    "\n",
    "#We just setted three Params:\n",
    "# - featuresCol: the column containing our features vector\n",
    "# - labelCol: the column containing our label (or target)\n",
    "# - predictionCol: the column that will contain the predictions of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004b6d82-d667-4668-a2fc-f9297ca360bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below, we `fit()` our pipeline. Before doing so, we need to split our data set into two portions using `randomSplit()`: one for training, which\n",
    "we feed to our pipeline, and one for testing, which is what we use to evaluate our\n",
    "model fit.\n",
    "\n",
    "But before fitting our pipeline, we cache() the training data frame. We do this because ML uses the data frame\n",
    "repeatedly, so caching in memory provides an increase in speed if your cluster **has\n",
    "enough memory**.\n",
    "\n",
    "*Although PySpark will use the same seed, which should guarantee\n",
    "that the split will be consistent across runs, there are some cases where PySpark\n",
    "will break that consistency. If you want to be 100% certain about your splits,\n",
    "split your data frame, write each one to disk, and then read them from the\n",
    "disk location.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a937989-6ff2-4d77-b714-5f49a47d6655",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#This CMD may take up to 10min\n",
    "\n",
    "train, test = food.randomSplit([0.7, 0.3], 42) #42 is a seed\n",
    "\n",
    "train.cache()\n",
    "\n",
    "food_pipeline_model = food_pipeline.fit(train)\n",
    "results = food_pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6bc92e-e66b-43c1-a295-d979d3acbee8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results.select(\"prediction\", \"rawPrediction\", \"probability\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f6dc81-87d3-43c4-a123-9dbf28a2bd67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.3 Evaluate and optimize\n",
    "\n",
    "In this section, we perform a reviewing of our model results and tuning their implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5d87ed-0287-471e-83e4-f9d7c4c7d9c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####4.3.1 Assessing model accuracy: Confusion matrix and evaluator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ac8fbec-f4d3-40c6-a7ca-0644bca33abb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#This CMD may take up to 1 min\n",
    "\n",
    "results.groupby(\"dessert\").pivot(\"prediction\").count().show()\n",
    "\n",
    "#The confusion matrix shows that our data set has a lot more non-desserts than desserts. In the classification world this is called an imbalanced data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93be1b6-07a2-4ca2-a68f-f5f4587d8650",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In Spark 3.1, we now have access to a new `LogisticRegressionSummary` object that avoids the trip to\n",
    "the RDD world.\n",
    "\n",
    "We need to first extract our fitted model\n",
    "from the pipeline model. For this, we can use the stages attribute of `pipeline_\n",
    "food_model` and access just the last item. From that model, called `lr_model` in the CMD below, we call `evaluate()` on the results data set. `evaluate()` will error out any prediction\n",
    "columns that exist, so I simply give the relevant ones (dessert, features) to\n",
    "it. It‚Äôs a small price to pay to avoid computing the metrics by hand. Note that PySpark\n",
    "does not know which label we consider positive and negative. Because of this, the precision\n",
    "and recall are accessible through `precisionByLabel` and `recallByLabel`,\n",
    "which both return lists of precision/recall for each label in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af960bf-1f82-4765-93f9-44fdfe1ff218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_model = food_pipeline_model.stages[-1]\n",
    "metrics = lr_model.evaluate(results.select(\"title\", \"dessert\", \"features\"))\n",
    "\n",
    "# LogisticRegressionTrainingSummary\n",
    "\n",
    "print(f\"Model precision: {metrics.precisionByLabel[1]}\") \n",
    "print(f\"Model recall: {metrics.recallByLabel[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65173203-d6e8-4b2a-a8d5-3e90edae23ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The receiver operating characteristic curve (ROC) is another common metric used when evaluating binary classification\n",
    "models. \n",
    "\n",
    "The ROC curve is obtained through the BinaryClassificationEvaluator object.\n",
    "Below we instantiate the said object, asking explicitly for the areaUnderROC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766730b5-9b50-4fad-bc9d-82d108a8956a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#As homework, you may try build this ROC curve using matplotlib\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"dessert\",  # <1>\n",
    "    rawPredictionCol=\"rawPrediction\",  # <1>\n",
    "    metricName=\"areaUnderROC\",\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(results)\n",
    "print(f\"Area under ROC = {accuracy} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19a3a71-1993-4757-a0e1-2ef17c1add02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 4.3.2 Optimizing hyperparameters with cross-validation\n",
    "\n",
    "By fine-tuning some aspects of the model training (how Spark builds the\n",
    "fitted model), we can hope to yield better model accuracy. For this, we use a technique\n",
    "called cross-validation. Cross-validation resamples the data set into training and\n",
    "testing sets to assess the ability of the model to generalize over new data.\n",
    "\n",
    "To build the set of hyperparameters we wish to evaluate our model against, we use the\n",
    "ParamGridBuilder, which assists in creating a Param Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e0a874-451d-4498-9d15-9f365d334153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "grid_search = (\n",
    "    ParamGridBuilder() \n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1.0]) \n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4492da-7845-4f17-8aa3-9a533de0cb2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "the output may be a messy, so to facilitate the reading:\n",
    "\n",
    " [\n",
    " \n",
    "     {Param(parent='LogisticRegression_14302c005814',\n",
    "            name='elasticNetParam',\n",
    "            doc='...'): 0.0},  <4>\n",
    "     {Param(parent='LogisticRegression_14302c005814',\n",
    "            name='elasticNetParam',\n",
    "            doc='...'): 1.0}  <4>\n",
    "\n",
    "]\n",
    "\n",
    "Now onto cross-validation. PySpark provides out-of-the-box K-fold crossvalidation\n",
    "through the CrossValidator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8347dee3-fb00-4d1b-981d-e90899588aa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ATTENTION: This command may take 1h or more. Only run the cell if you have patience or can do something else in the meantime. \n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=food_pipeline,\n",
    "    estimatorParamMaps=grid_search,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=13,\n",
    "    collectSubModels=True,\n",
    ")\n",
    "\n",
    "# cv_model = cv.fit(train)\n",
    "\n",
    "# print(cv_model.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "087fff8f-4654-4135-994b-a8c78e9f24c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pipeline_food_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d5b6066-458f-43e7-b18b-ab7c4b736779",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.3 Extracting the coefficientes\n",
    "\n",
    "This section covers the extraction of our model features and their coefficients. We use\n",
    "those coefficients to get a sense of the most important features of the model and plan\n",
    "some improvements for a second iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f2a649-06bd-4149-aed7-0cc18df366fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# feature_names = [\"(Intercept)\"] + [ x[\"name\"]\n",
    "#     for x in (\n",
    "#         food_features\n",
    "#         .schema[\"features\"]\n",
    "#         .metadata[\"ml_attr\"][\"attrs\"][\"numeric\"]\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# feature_coefficients = [lr_model.intercept] + list(\n",
    "#     lr_model.coefficients.values\n",
    "# )\n",
    "\n",
    "\n",
    "# coefficients = pd.DataFrame(\n",
    "#     feature_coefficients, index=feature_names, columns=[\"coef\"]\n",
    "# )\n",
    "\n",
    "# coefficients[\"abs_coef\"] = coefficients[\"coef\"].abs()\n",
    "\n",
    "# print(coefficients.sort_values([\"abs_coef\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5284dc-b500-427a-8bdf-34396079de06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A coefficient close to zero, like kirsch, lemon, and food_processor, means that\n",
    "this feature is not very predictive of our model. On the flip side, a very high or low\n",
    "coefficient, like cauliflower, horseradish, and quick_and_healthy, means that this\n",
    "feature is highly predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcffb1f2-410b-44b4-80c8-b2188793425e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "We are going to create a model to predict the flight delay over 15 minutes (```ARR_DEL15```) using other attributes - such as, airport code, career, and various weather conditions.\n",
    "\n",
    "Before starting, you must download the dataset `flight_weather.csv` You will find the dataset on Moodle. Since this is a tabular dataset, you can go to ``Catalog``, then ``tables``. There you can create the table (using the UI option is fine).\n",
    "\n",
    "> Note : For more accurate learning in classification, use LightGBM classifier in SynapseML library (formerly MMLSpark library).<br>\n",
    "> Here I use built-in DecisionTree Classifier in MLlib.\n",
    "\n",
    "*This exercise was based on https://github.com/tsmatz/azure-databricks-exercise*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31eadcad-0e14-4881-bbac-89002bba3dda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Import and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14ed493-ab9b-45d5-93eb-b4f06c68131c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IMPORT DATASET\n",
    "\n",
    "# File location and type\n",
    "file_location = \"/FileStore/flight_weather.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b398142-d02e-4610-915b-378a2201a494",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this dataset,\n",
    "\n",
    "`ARR_DEL15` : 1 when the flight is delayed over 15 minutes, 0 otherwise.\n",
    "\n",
    "`XXXOrigin` : Weather conditions in departure airport.\n",
    "\n",
    "`XXXDest` : Weather conditions in destination airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f218a73-f303-4aff-b69a-77ecc6fec1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Code here: NUMBER OF ROWS/COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff408b63-c5c5-405c-9e03-6e626e96c658",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Code here: PRINT SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527538ba-6856-4a08-b67f-519fb63f2ec1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Explore the features\n",
    "\n",
    "You may like to explore the feature by using the graphics in the table above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5788efd9-a04b-4df8-8cab-1ba60d874587",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Data mishapes and feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc204d8-c901-4bc6-877d-bce9294e70e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Mark as \"delayed over 15 minutes\" if it's canceled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "384f38d3-d087-4ea4-855b-3548a400473f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15ab388-8103-48cb-9743-1304c7245f6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Remove flights if it's diverted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684929a5-4611-491e-ad0c-4d05f82171fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4cd62b-76e4-4fdf-990a-f7af5c99f903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df \\\n",
    "    .withColumns(\n",
    "        {\n",
    "            \"RelativeHumidityOrigin\": df[\"RelativeHumidityOrigin\"].cast(IntegerType()),\n",
    "            \"AltimeterOrigin\": df[\"AltimeterOrigin\"].cast(IntegerType()),\n",
    "            \"DryBulbCelsiusOrigin\": df[\"DryBulbCelsiusOrigin\"].cast(IntegerType()),\n",
    "            \"WindSpeedOrigin\": df[\"WindSpeedOrigin\"].cast(IntegerType()),\n",
    "            \"VisibilityOrigin\": df[\"VisibilityOrigin\"].cast(IntegerType()),\n",
    "            \"DewPointCelsiusOrigin\": df[\"DewPointCelsiusOrigin\"].cast(IntegerType()),\n",
    "            \"RelativeHumidityDest\": df[\"RelativeHumidityDest\"].cast(IntegerType()),\n",
    "            \"AltimeterDest\": df[\"AltimeterDest\"].cast(IntegerType()),\n",
    "            \"DryBulbCelsiusDest\": df[\"DryBulbCelsiusDest\"].cast(IntegerType()),\n",
    "            \"WindSpeedDest\": df[\"WindSpeedDest\"].cast(IntegerType()),\n",
    "            \"VisibilityDest\": df[\"VisibilityDest\"].cast(IntegerType()),\n",
    "            \"DewPointCelsiusDest\": df[\"DewPointCelsiusDest\"].cast(IntegerType()),\n",
    "            \"ARR_DEL15\", df[\"ARR_DEL15\"].cast(IntegerType())\n",
    "        }\n",
    "    )\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f07b83-ca3d-468d-ba95-ce902f64b5ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.4 Find and delete useless records and input binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d1e850-e288-4ee8-92f7-d1214512eee3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Narrow to required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4afbd3-6734-45c3-bd02-67ee423e08ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.select(\n",
    "  \"ARR_DEL15\",\n",
    "  \"MONTH\",\n",
    "  \"DAY_OF_WEEK\",\n",
    "  \"UNIQUE_CARRIER\",\n",
    "  \"ORIGIN\",\n",
    "  \"DEST\",\n",
    "  \"CRS_DEP_TIME\",\n",
    "  \"CRS_ARR_TIME\",\n",
    "  \"RelativeHumidityOrigin\",\n",
    "  \"AltimeterOrigin\",\n",
    "  \"DryBulbCelsiusOrigin\",\n",
    "  \"WindSpeedOrigin\",\n",
    "  \"VisibilityOrigin\",\n",
    "  \"DewPointCelsiusOrigin\",\n",
    "  \"RelativeHumidityDest\",\n",
    "  \"AltimeterDest\",\n",
    "  \"DryBulbCelsiusDest\",\n",
    "  \"WindSpeedDest\",\n",
    "  \"VisibilityDest\",\n",
    "  \"DewPointCelsiusDest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5407e0ec-e279-401f-b20e-b8767de75608",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Drop rows which has null value in columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28365cd7-76b7-43e1-a71a-8fa103f1abb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Code here: DROPNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8491046f-c9e6-40ac-9021-86f1890e335c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Code here: SHOW RESULTS AFTER DROPNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c490583-b8d6-4c73-96dc-f5223c1e8dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.5 Cleaning continuous variables (and extreme values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2e92f4-5313-4067-b5ef-2826e8a0c378",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Look for extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06bb3106-c66b-460e-98f5-cc9cc7bf2d35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summ = df.select(\"ARR_DEL15\", \"AltimeterDest\", \"WindSpeedDest\", \"AltimeterOrigin\", \"VisibilityDest\", \"WindSpeedOrigin\", \"VisibilityOrigin\", \"DryBulbCelsiusDest\", \"DewPointCelsiusDest\", \"DryBulbCelsiusOrigin\", \"RelativeHumidityDest\", \"DewPointCelsiusOrigin\", \"RelativeHumidityOrigin\").summary(\n",
    "\"mean\",\n",
    "\"stddev\",\n",
    "\"min\",\n",
    "\"1%\",\n",
    "\"5%\",\n",
    "\"50%\",\n",
    "\"95%\",\n",
    "\"99%\",\n",
    "\"max\",\n",
    ")\n",
    "\n",
    "display(summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9cab8f-24ab-4360-938d-ec739950d643",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####4.2.1 Final dataset (vector column type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfab3cb-fde4-48a3-81d4-372b4f83c745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train data and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50089cc5-991a-4d72-a03d-4296eef0448d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Convert categorical values to index values (0, 1, ...) for the following columns.\n",
    "\n",
    "- Carrier code (```UNIQUE_CARRIER```)\n",
    "- Airport code in departure (```ORIGIN```)\n",
    "- Airport code in destination (```DEST```)\n",
    "- Flag (0 or 1) for delay over 15 minutes (```ARR_DEL15```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db14ba9-c00e-430f-9a08-766f3bedec4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "uniqueCarrierIndexer = StringIndexer(inputCol=\"UNIQUE_CARRIER\", outputCol=\"Indexed_UNIQUE_CARRIER\").fit(df)\n",
    "originIndexer = StringIndexer(inputCol=\"ORIGIN\", outputCol=\"Indexed_ORIGIN\").fit(df)\n",
    "destIndexer = StringIndexer(inputCol=\"DEST\", outputCol=\"Indexed_DEST\").fit(df)\n",
    "arrDel15Indexer = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"Indexed_ARR_DEL15\").fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c1d7ce-95c7-47df-8444-2ea45d84f8e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble feature columns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "  inputCols = [\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"Indexed_UNIQUE_CARRIER\",\n",
    "    \"Indexed_ORIGIN\",\n",
    "    \"Indexed_DEST\",\n",
    "    \"CRS_DEP_TIME\",\n",
    "    \"CRS_ARR_TIME\",\n",
    "    \"RelativeHumidityOrigin\",\n",
    "    \"AltimeterOrigin\",\n",
    "    \"DryBulbCelsiusOrigin\",\n",
    "    \"WindSpeedOrigin\",\n",
    "    \"VisibilityOrigin\",\n",
    "    \"DewPointCelsiusOrigin\",\n",
    "    \"RelativeHumidityDest\",\n",
    "    \"AltimeterDest\",\n",
    "    \"DryBulbCelsiusDest\",\n",
    "    \"WindSpeedDest\",\n",
    "    \"VisibilityDest\",\n",
    "    \"DewPointCelsiusDest\"],\n",
    "  outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa351df-a9e0-462a-8616-c44a57760f60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.2.2 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c9cc2c7-e25d-4bab-8a31-099bb487e72c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate classifier (Decision tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35eaa177-d903-41fd-a728-930205ca6126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create pipeline and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21176acb-f573-4013-87f8-b794375a6f4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict with eveluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3313992-a9f6-403e-9437-0bf6e005515d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####4.3.1 Assessing model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920aa861-f0a6-4523-bfb1-acdabf627c22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce62abee-cd7a-49a1-ae87-219c6bd57c17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store pipeline"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab12_bda_class",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
