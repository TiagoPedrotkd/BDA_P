{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8331e569-f4d3-4a05-b058-412e374aca7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark Machine learning - Spark-MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6870f217-64f1-499a-90bf-5a700929fa73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As organizations create more diverse and more user-focused data products and services, there is a growing need for machine learning, which can be used to develop personalizations, recommendations, and predictive insights. The Apache Spark machine learning library (MLlib) allows data scientists to focus on their data problems and models instead of solving the complexities surrounding distributed data (such as infrastructure, configurations, and so on).\n",
    "\n",
    "From the inception of the Apache Spark project, MLlib was considered foundational for Spark’s success. The key benefit of MLlib is that it allows data scientists to focus on their data problems and models instead of solving the complexities surrounding distributed data (such as infrastructure, configurations, and so on). The data engineers can focus on distributed systems engineering using Spark’s easy-to-use APIs, while the data scientists can leverage the scale and speed of Spark core. Just as important, Spark MLlib is a general-purpose library, providing algorithms for most use cases while at the same time allowing the community to build upon and extend it for specialized use cases.\n",
    "\n",
    "This week we will look at the basics of some of the spark MLlib packages, next week we build these into machine learning pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "255e9cce-e7c7-48e5-8b9f-bc15fcb374f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will start with a databricks database that compares city population to median sale prices of homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f8eec93-4f7c-4662-8cb8-6af4f53be9f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import col\n",
    "# Use the Spark CSV datasource with options specifying:\n",
    "# - First line of file is a header\n",
    "# - Automatically infer the schema of the data\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/databricks-datasets/samples/population-vs-price/data_geo.csv\")\n",
    "data.cache() # Cache data for faster reuse\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05a08ff-c80f-491f-a3eb-e504f35b341c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Take a look at the data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3de7e295-8af5-40fa-be59-45a0f384cd17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.printSchema()\n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f19226-94fd-4381-b023-a1bd6b8ed7c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we clean the data by dropping any rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "999f6094-564b-4665-9030-ee8a21f06b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = data.dropna() # drop rows with missing values\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b458d00-650d-4174-9dc8-500a0f22aa55",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we select the columns we will use to predict house prices (features) and the correct output (labels). Below we use the databricks display function which can be useful for examining our dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "154d2fc5-b758-4fa0-9c16-8630bdc91f94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataFeat = data.select([\"2014 Population estimate\",\"2015 median sales price\"])\n",
    "display(dataFeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32ad97c2-d803-45f5-b6dc-0ddb75e61c67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally we need to convert the data to the format expected by spark, a vector of features as a single column and the label as an additional column.\n",
    "\n",
    "We define a function that does this (which we can use later). As you can see it takes our DataFrame and converts it to an RDD. This then allows us to apply the map transformation to each entry of the RDD. Here we take all the columns except the last (in our case only one) and convert these to a dense vector, we keep the last column seperate for the label.\n",
    "\n",
    "Vectors (dense or sparse) are a way of holding data that is used a lot in machine learning. In the features column below we only have one data item (the size of the population) but as you can see the data structure is as follows: [1,1,[],[188226]] this format makes more sense when we use sparse vectors (it easier to see the data is one of the advantages, but mostly its is more space efficient) for more information this video has a short explanation of the vector format: https://www.youtube.com/watch?v=oGwEv82ifrE\n",
    "\n",
    "Here is the same vector in three formats:\n",
    "```python\n",
    "##### Use a NumPy array as a dense vector.\n",
    "dv1 = np.array([1.0, 0.0, 3.0])\n",
    "##### Use a Python list as a dense vector.\n",
    "dv2 = [1.0, 0.0, 3.0]\n",
    "##### Create a SparseVector.\n",
    "```\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0]) ```\n",
    "You will find out that most of the machine learning algorithms in Spark are based on the features and label format. That is to say, you can play with all of the machine learning algorithms in Spark aftre you prepare your data in this format with features and label.\n",
    "\n",
    "The function defined below will take a row of our dataframe and convert all of the features into a dense vector. The final dataformat is a dataframe with two columns, a label column and a features column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bda8433f-e50d-4735-ad26-d9759079974d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "https://spark.apache.org/docs/latest/ml-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66dae65b-27ee-4c31-bb2f-62e01723fa86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfb70061-7a01-49d2-8c2c-8615840f32c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a = dataFeat.rdd.map(lambda x: Vectors.dense(x))\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c51510d4-ca8d-4c5e-a7dc-e7fc52f585f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transData(data):\n",
    "  # Combine columns to a dense vector (excluding the last column)\n",
    "  dataFeaturesRDD = data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]])\n",
    "  \n",
    "  # Convert the RDD back to a DataFrame, labelling the columns\n",
    "  featuresDF =  dataFeaturesRDD.toDF(['features','label'])\n",
    "  \n",
    "  return featuresDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95fbc8ab-bc77-4466-8559-d07adec2a06c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataLR = transData(dataFeat)\n",
    "dataLR.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb1432c2-23fb-47c8-b159-7114857c31e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally we can run our linear regression. Here we create two models with different parameters and print the coefficients they provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f812b571-eb04-4f51-b850-512713d938d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit 2 models, using different regularization parameters\n",
    "modelA = lr.fit(dataLR, {lr.regParam:0.0})\n",
    "modelB = lr.fit(dataLR, {lr.regParam:100.0})\n",
    "\n",
    "# Print the fitted model parameters\n",
    "print(\">>>> ModelA intercept: %r, coefficient: %r\" % (modelA.intercept, modelA.coefficients[0]))\n",
    "print(\">>>> ModelB intercept: %r, coefficient: %r\" % (modelB.intercept, modelB.coefficients[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77bf284f-4434-4de4-9581-d2ed2102002f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sometimes we will want to see the significance of the coefficients we have. We can check the p values of our regressors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d9d1afd-3615-41e3-ba23-850e3a3c1552",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seA = modelA.summary.pValues\n",
    "seB = modelB.summary.pValues\n",
    "\n",
    "print(\">>>> ModelA p value intercept: %r, p value coefficient: %r\" % (seA[1], seA[0]))\n",
    "print(\">>>> ModelB p value intercept: %r, p value coefficient: %r\" % (seB[1], seB[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ee1bab-e8f1-451b-b7d1-64444b056fcc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Generating predictions based on the model is easy, here we use the input data and compare model predicted to actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d957c58-8673-44c1-a90a-ed8774ddcb82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictionsA = modelA.transform(dataLR)\n",
    "predictionsB = modelB.transform(dataLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee1d7f1e-cf2b-496c-a657-8426867e7706",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predictionsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "843153a3-df5a-4937-9951-10e35b849caa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predictionsB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f69912f7-201e-496c-be16-35fa8182e88a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The predictions dont look too great, in databricks we can easily plot the residuals. You can drag the chart to make it bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f07769af-f4d9-4b3d-a56c-223e8bc0444b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(modelB, dataLR)\n",
    "#they should have a random distribution around 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44f1c60d-2855-40e4-87a8-0f26f9971bf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Additionaly, Spark includes functionality to help us with our models, for example the regression evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84e8f965-2a98-4bd9-be3f-caa8a4be5c58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "RMSE_A = evaluator.evaluate(predictionsA)\n",
    "RMSE_B = evaluator.evaluate(predictionsB)\n",
    "\n",
    "print(\"ModelA: Root Mean Squared Error = \" + str(RMSE_A))\n",
    "print(\"ModelB: Root Mean Squared Error = \" + str(RMSE_B))\n",
    "\n",
    "#Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d83485cf-b278-41d0-81fb-66b9abb365ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "According to RMSE ModelA performs better.\n",
    "\n",
    "\n",
    "### Practice exercise\n",
    "\n",
    "In the below two cells we download a database of car performance (efficiency in miles per gallon MPG) and a number of car characteristics. Use the linear regression estimator to try to predict miles per gallon MPG based on:\n",
    "\n",
    " - Just <b> weight </b> as a feature\n",
    "\n",
    " - <b> displacement, weight, acceleration, and year </b> as features\n",
    "\n",
    "Compare the RMSE of your the two aproaches and plot the residuals.\n",
    "\n",
    "Hint: the `transData` function we defined is very useful, however, it requires the final column to be the data we are trying to predict. You will need to either change this function or re-arrange the dataframe, as current, MPG is the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46860412-1e41-4ff8-acab-79b02e03ef99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh wget https://raw.githubusercontent.com/plotly/datasets/master/auto-mpg.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14455f26-e3b0-4473-890b-f72144ceca0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataMPG = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"file:/databricks/driver/auto-mpg.csv\")\n",
    "dataMPG.cache()\n",
    "print(dataMPG.count())\n",
    "dataMPG.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e83f7708-80a8-4b97-9d07-37baf1d25c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dataMPG.select([dataMPG.columns[2], dataMPG.columns[4], dataMPG.columns[5],  dataMPG.columns[6], dataMPG.columns[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d04fc1a-c262-40f1-920a-2d87223cf8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create the vector data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ced82cf-5131-431d-9446-bea57bdaf1a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create vector data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d04f1db8-6012-4ee3-b008-394c3c074d32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#fit models and create predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04faeff-7ec7-42f0-9cb7-16b34fb689e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Print intercept and coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9436ee2-b1c2-4e58-b4d1-c03d87782c81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Print RMSE's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27461558-f2ea-4244-813a-5097bbf58d49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Gradient Boosted Tree Regression\n",
    "\n",
    "Lets try another model from the spark toolkit on our data: Gradient Boosted Tree Regression https://statisticasoftware.wordpress.com/2012/09/11/boosting-trees-for-regression-and-classification/\n",
    "\n",
    "Follow the steps below to see how easily we can apply a different machine learning approach to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f80da1c-436a-4b9b-86aa-eb3ebb7c6728",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "rf = GBTRegressor(maxDepth=2, seed=42) #1st try: maxDepth=2, seed=42 | 2nd try: maxDepth=2, seed=42 \n",
    "dataMPGOrdered = transData(dataMPG.select(dataMPG.columns[1:] + [dataMPG.columns[0]]  ))\n",
    "rf.setMaxIter(10) #1st try: 10 | 2nd try: 30\n",
    "rf.setMinWeightFractionPerNode(0.1)  #1st try: 0.1 | 2nd try: 0.005\n",
    "modelG = rf.fit(dataMPGOrdered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dab086b-df04-4bfe-8e9a-b6e6212931fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = modelG.transform(dataMPGOrdered)\n",
    "predictions.select(\"features\",\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ebd085b-1f11-42fb-b727-56d3413a15b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "#1st 3.12151  | 2nd 0.847873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e878244c-1ad2-4382-98d4-40c7a7c52338",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "How did this approach perform in comparison to the previous regressions? Remember we are only looking at RMSE on the training data.\n",
    "\n",
    "This model is prone for overfitting, below we split the data into a training and test dataset. This would be standard good practice if this is all of the data we have.\n",
    "\n",
    "Fit the model to the training dataset and test the fit against the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "114b8255-96a6-49c5-88bf-7872a57f42c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = dataMPGOrdered.randomSplit([0.6, 0.4])\n",
    "\n",
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68ad65a-6da5-4757-86dd-5d7c31438cd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelGTr = rf.fit(trainingData)\n",
    "predictionsGTr = modelGTr.transform(testData)\n",
    "rmseGtr = evaluator.evaluate(predictionsGTr)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmseGtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0d3ba9f-763c-4637-9f0a-d7d3e04c7aae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bonus exercise: Machine learning practice\n",
    "\n",
    "I've included below a dataset of wine characteristics and quality, you can use this to practice applying the aprroachs above to build a predictor of wine quality.\n",
    "\n",
    "Try to find the best model you can to predict wine quality.\n",
    "\n",
    "If you want, you can try other machine learning models, see: https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#regression\n",
    "\n",
    "##### Below I fit two basic models to the data, based on the methods we have seen in this lab, and compare them. However, there is a lot more you could do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c37d395a-8f5b-44d4-a8cb-3dd60bc8526f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh wget https://raw.githubusercontent.com/MingChen0919/learning-apache-spark/master/data/WineData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9330b101-7178-44c6-8355-fe504b966f07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wineDF = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"file:/databricks/driver/WineData.csv\")\n",
    "wineDF.cache()\n",
    "wineDF.printSchema()\n",
    "wineDF.select(\"quality\").distinct().take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "804912bb-1456-4842-8a0c-777bd56c6581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to float format\n",
    "def string_to_float(x):\n",
    "    return float(x)\n",
    "\n",
    "#\n",
    "def condition(r):\n",
    "    if (0<= r <= 4):\n",
    "        label = \"low\"\n",
    "    elif(4< r <= 6):\n",
    "        label = \"medium\"\n",
    "    else:\n",
    "        label = \"high\"\n",
    "    return label\n",
    "  \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "string_to_float_udf = udf(string_to_float, DoubleType())\n",
    "quality_udf = udf(lambda x: condition(x), StringType())\n",
    "wineDFQ = wineDF.withColumn(\"quality\", quality_udf(\"quality\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b996ae7-4945-4f28-9bb7-75a9c98b432b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Suggested steps:\n",
    "#   1. Look the dataset\n",
    "#   2. Create the vector\n",
    "#   3. Split train e test\n",
    "#   4. Predict\n",
    "#   5. Look the RMSE"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "bda_lab11_class",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
