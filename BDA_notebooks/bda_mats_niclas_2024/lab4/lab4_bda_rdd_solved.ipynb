{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbc0c36-34b9-4502-9c70-1193537d5789",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# BD Lab - Spark practice\n",
    "\n",
    "Apache Spark is a lightning-fast cluster computing technology, designed for fast computation. It is based on Hadoop MapReduce and it extends the MapReduce model to efficiently use it for more types of computations, which includes interactive queries and stream processing. The main feature of Spark is its in-memory cluster computing that increases the processing speed of an application.\n",
    "\n",
    "Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries and streaming. Apart from supporting all these workload in a respective system, it reduces the management burden of maintaining separate tools.\n",
    "\n",
    "Apache Spark has the following features.\n",
    "\n",
    "Speed − Spark helps to run an application on a Hadoop cluster up to 100 times faster than in-memory, and 10 times faster when running on disk. This is possible by reducing the number of read/write operations to disk. It stores the intermediate processing data in memory.\n",
    "\n",
    "Supports multiple languages − Spark provides built-in APIs in Java, Scala, or Python. Therefore, you can write applications in different languages. Spark comes up with 80 high-level operators for interactive querying.\n",
    "\n",
    "Advanced Analytics − Spark not only supports ‘Map’ and ‘reduce’. It also supports SQL queries, Streaming data, Machine learning (ML), and Graph algorithms.\n",
    "\n",
    "## Background Spark Context\n",
    "\n",
    "There are a few spark concepts that we will now introduce before we get started. The sparkcontext object which allows us to interact with spark and the spark data structure of RDD.\n",
    "Sparkcontext is basically just an entry point to any Spark functionality. Spark applications are run as independent sets of processes, coordinated by a Spark Context in a driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ec1aeb-f571-486d-861b-4ce3947aeae5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Title](https://annefou.github.io/pyspark/slides/images/SparkRuntime.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1270ef1a-8e30-4d53-9f97-e532ba108631",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: '3.3.2'"
     ]
    }
   ],
   "source": [
    "#check the Spark version you are running and the Databricks version\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91d7dc74-d89c-451b-819e-cc64720df6ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The spark context may be automatically created (for instance if you call pyspark from the shells (the Spark context is then called sc).\n",
    "\n",
    "You will see we will use the sparkcontext by invoking methods on the sc object to interact with spark.\n",
    "\n",
    "QUICK NOTE: The spark context is actually replaced by the spark session in later versions of spark. We will talk about this in the following labs, but using the spark context now will mean you have familiarity with both.\n",
    "\n",
    "## Background on RDD\n",
    "\n",
    "We will start by looking at the spark concept of Resilient Distributed Datasets (RDD). RDD is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
    "\n",
    "Formally, an RDD is a <b> read-only, partitioned </b> collection of records. RDDs can be created through deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "\n",
    "There are two ways to create RDDs − parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.\n",
    "\n",
    "You can apply multiple operations on these RDDs to achieve a certain task. To apply operations on these RDD's, there are two ways −\n",
    "\n",
    "<b>Transformation</b> − These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations.\n",
    "\n",
    "<b>Action</b> − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver.\n",
    "\n",
    "<b>Laziness of Spark transformations </b>\n",
    "It’s important to understand that transformations are evaluated lazily, meaning computation doesn’t take place until you invoke an action. Once an action is triggered on an RDD, Spark examines the RDD’s lineage and uses that information to build a “graph of operations” that needs to be executed in order to compute the action. Think of a transformation as a sort of diagram that tells Spark which operations need to happen and in which order once an action gets executed. We will work with examples that demonstrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24efeeab-08d2-432b-9df6-909b38783614",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark - Lazy evaluation\n",
    "\n",
    "Apache Spark is written in Scala programming language. To support Python with Spark, Apache Spark Community released a tool, PySpark. Using PySpark, you can work with RDDs in Python programming language also.\n",
    "\n",
    "PySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context. The majority of data scientists and analytics experts today use Python because of its rich library set. Integrating Python with Spark is a boon to them.\n",
    "\n",
    "## Lazy evaluation and caching\n",
    "\n",
    "In this example we will take a quick look at what lazy execution means for our code and how caching works. Take a look at the function we will use below (and run the cell). As you can see it is designed to take time to evaluate, this function just waits one second before continuing. Basically, we are representing a task we want to do that takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4adecce5-438b-4cb7-ad27-e4782ecbe2ff",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create a fucntion called slowLoad that we will use later\n",
    "def slowLoad(x):\n",
    "    time.sleep(1) # sleeps for 1 second\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c37257-e040-4148-aad3-baf51e01048c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's define a quick dataset to work with, first we create data in Python. Then we will create an RDD based on this input. To do so we use the spark context object to parallelize the data. To return the data we use the collect() action which evaluates the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622c437a-2c1e-44d1-8d21-bb3e6206378e",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
     ]
    }
   ],
   "source": [
    "#create Python data (a list of the numbers 0 to 19)\n",
    "dataPython = range(0,20)\n",
    "\n",
    "#load the data into an RDD by spreading it out into our big data cluster\n",
    "smallRDD = sc.parallelize(dataPython,1)\n",
    "\n",
    "#return the data from the RDD using an 'action' called collect\n",
    "smallRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ea3657-b5b3-4b4c-9a7b-7296e0c3e82a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we create a new RDD that uses our function. We use a map function (this is a transformation, we will talk more about this next), as with the map reduce framework we started with. Map allows us to apply any function to all our data. Here we are simulating a data transformation that takes a long time. For every item in our database, the function will take 1 second to evaluate (approximately). So with 20 items in our dataset applying the function 20 times should take 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c997eb90-4044-4a03-8b45-844290ba91fe",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use the map transformation to apply the function we made to the data\n",
    "newRDD = smallRDD.map(slowLoad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e7c3597-ea91-494a-9293-006780ac3558",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That cell probably evaluated quicker than you expected. In my case it took 0.04 seconds but the function we created should take 1 second for each item in our database with 20 items.\n",
    "\n",
    "What do you think is currently in the new RDD we have just created that is called newRDD?\n",
    "\n",
    "Right now nothing (or at least no data), but as soon as we try to look at the contents (or otherwise use the data) spark will need to perform the computation to create the new RDD. This is the difference between transformations (such as map) and actions (such as collect). So let’s apply the collect action to see what is in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b395f714-e1e1-4818-9644-e173d8acf09c",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
     ]
    }
   ],
   "source": [
    "# Use the collect action to resturn the contents of our data\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43c820b-aebc-4af7-8356-3334bdd43191",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You should see that only now was our function actually being used (as it takes time), not when we created the newRDD, as only now was an action used.\n",
    "\n",
    "Hopefully you can see this is completely different to running cells in Python normally.\n",
    "\n",
    "Run the above cell **TWICE** to check that it takes the same amount of time (that in both cases our data transformation is being recalculated).\n",
    "\n",
    "So a couple of key take-aways. We use transformations to apply functions to our data, for example we could have a function that will clean the data and use map to apply it. When we use the transformation we get a new RDD that has had the function applied to all of the data, so in our data cleaning example we would get a new RDD with clean data. **BUT** the new RDD doesn't actually contain any data, just the instructions for how to create the cleaned data. We can apply many transformations and no actual new data will have been created. But when we want to see the results of course the instructions to change the data will need to be executed, so when we perform an 'action', such as collect, which will return a value all the transformations will be applied.\n",
    "\n",
    "Often we will want to work with transformed data in multiple queries, for example we might want to use our cleaned data multiple times without having to redo the calculations to clean the data multiple times. Or alternatively, we might want to use an iterative algorithm (such as training machine learning) and we don't want to have to redo all previous transformations in each iteration. In these cases we can 'persist' the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f317cd70-7f52-406d-a4f9-5660e02ce7ad",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: PythonRDD[2] at wrapper at <command-734436448619080>:2"
     ]
    }
   ],
   "source": [
    "# Tell the RDD to store the results, not just the instructions for creating the results, for future use using the cache function.\n",
    "newRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0bb966c-932f-4985-be78-e3b0f143b1a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the below cell **THREE TIMES** to see the difference. The first time should take 20 seconds again. But after the first time the data the data is calculated it will be held in memory so that it can be quickly retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6ca2e5-e85c-4128-a308-19a6a5c9d2da",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
     ]
    }
   ],
   "source": [
    "# Use the collect action to get the contents of the RDD\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f8b2ab-530f-4c47-87b8-f94430106ec1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the above case we have persisted the RDD to memory (RAM) we can also persist to disk (or a combination of the two). See https://data-flair.training/blogs/apache-spark-rdd-persistence-caching/ for more information.\n",
    "\n",
    "In the above example we have been working with the basic data structure of spark: the RDD. You have seen how to create an RDD by parallelizing some existing python data and you have seen how to get results out by using an **action** 'collect' which returns the values from an RDD. There is another important operation we will be using with RDDs and that is the **transformation** which creates a new RDD from an existing one, usually with some change we have made to the data (a transformation of the data). For example we map load out data into an RDD, then decide to clean the data by applying a transformation and getting a new RDD with clean data. Remember, RDDs are immutable, this means they cannot be changed, so we can't just clean the data inside the RDD we loaded the data into, as this would be changing the contents of the RDD which is not possible, we need to work with transformations. Two of the most common transformations are explained below.\n",
    "\n",
    "First we start by creating some data (a list with three strings) and parallelizing it into our cluster to create an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0d7062-40b5-465c-a3e8-5f7856ba539e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the data as a list in Python\n",
    "mylist = [\"This is the first line\", \"Now the second line\", \"Finally the third line\", \"The Hittite capital of Hattusa is located in modern-day Turkey\"]\n",
    "\n",
    "# Load the data into spark to create an RDD with the data inside\n",
    "myRDD = sc.parallelize(mylist)\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4272afb7-0e50-4fdc-a6ba-768577b3a510",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Take a look at the cell below, here I apply the map transformation to our RDD, the output of which is a new RDD which I have called 'splitListRDD'. The map transformation applies a function to all of the data in the RDD, here I have created a lambda function which will split the strings. To see what is inside the new RDD we then need to use the collect action to get the data out. Remember, that we have lazy execution so that when I applied the map transformation nothing actually happened, spark just keeps a record of \"how to create\" the data we want, it is only when we perform an action that any calculation is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c99e71-1c52-4402-8a3a-2374fc63d0e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#apply a funtion, in this case a lambda function that splits the words, with map to create a new RDD\n",
    "splitListRDD = myRDD.map(lambda line: line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65003c79-2079-42e9-a34a-6f1c3e470dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: [['This', 'is', 'the', 'first', 'line'],\n ['Now', 'the', 'second', 'line'],\n ['Finally', 'the', 'third', 'line'],\n ['The',\n  'Hittite',\n  'capital',\n  'of',\n  'Hattusa',\n  'is',\n  'located',\n  'in',\n  'modern-day',\n  'Turkey']]"
     ]
    }
   ],
   "source": [
    "#return the results from the new RDD\n",
    "splitListRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0282ba-0286-42cb-86de-90b5f8406c35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "So we see we have transformed our data, which was a list of three strings, into **a list of three lists: inside each of these three lists are the words split separately.** It is important to see that the number of items in the data set is the same, before we had three strings (three sentences), now we have three lists (lists of each word in the original sentence).\n",
    "\n",
    "This is important as it is the difference to the other most common transformation we will be using called flatMap. Like  map, flatMap will apply a function to all out data in the database, however, this function 'flattens' out all the data. So imagine we wanted all the words from our data, maybe to count them, but our previous transformation didn't quite help as we ended up with words all in different lists. The flatmap transformation is the answer in that it removes this structure (flattens out the data so it is all on the same level of the list). Take a look at the below example and see the difference in the result returned, now we have all our words together in a single list. Also notice how I 'chain' together the transformation and the action into one line, this is a common way to apply transformations all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1508b60c-ac7e-4bfc-8a38-5d80eab6ac90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: ['This',\n 'is',\n 'the',\n 'first',\n 'line',\n 'Now',\n 'the',\n 'second',\n 'line',\n 'Finally',\n 'the',\n 'third',\n 'line',\n 'The',\n 'Hittite',\n 'capital',\n 'of',\n 'Hattusa',\n 'is',\n 'located',\n 'in',\n 'modern-day',\n 'Turkey']"
     ]
    }
   ],
   "source": [
    "#apply a funtion, in this case a lambda function that list the words, with flatmap and return the results\n",
    "myRDD.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56bb902-f957-4c2f-8a6c-6ae5ffefe46a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's worth noting flatMap just removes one level of the data, take a look at the example below with lots of lists inside lists, flatmap has only removed the structure at one level. Take a look at 8 which was inside a list inside the main data list. After the flatmap 8 is just inside the data list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ed0a6d-af39-4761-b57c-b5d06c86d1d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: [[0, [1, 2, 3, 4, 5], 7], [8], [9, [10, [11, 12]]]]"
     ]
    }
   ],
   "source": [
    "#create the data\n",
    "dataTest = [[0,[1, 2, 3, 4, 5],7], [8], [9, [10,[11, 12]]]]\n",
    "\n",
    "#load the data into an RDD\n",
    "dataTestRDD = sc.parallelize(dataTest)\n",
    "\n",
    "#take a look at the contents of the RDD\n",
    "dataTestRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b2ffb1-9fe2-4d80-9040-1341984a3e38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: [0, [1, 2, 3, 4, 5], 7, 8, 9, [10, [11, 12]]]"
     ]
    }
   ],
   "source": [
    "#This line of code applies that flatmap and returns the values\n",
    "dataTestRDD.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f45f04-fefb-4b84-9dfd-3ff89fd2ded2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another popular transformation is filter that reduces the amount of data in the RDD to what we are interested in. For example in the code below I use filter and the lambda function to remove all numbers less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef57231-2b46-4370-8111-0dbf44f325d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [1, 2, 3, 4, 5, 6, 0]"
     ]
    }
   ],
   "source": [
    "# create the data and load into an RDD\n",
    "numberRDD = sc.parallelize([1,2,3,4,-1,-2,5,-5,6,0])\n",
    "\n",
    "# filter the data using the filter transformation, this creates a new RDD with the filtered data\n",
    "filteredRDD = numberRDD.filter(lambda x: x>=0)\n",
    "\n",
    "# return the contents of the filtered data\n",
    "filteredRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ad8acd-8cb0-45fb-8f3a-a8110978cae0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's look at a common mistake that people can make when using a map transfromation for the first time. I'm going to use a dataset with the number 0 to 19, and I'm going to create a function that will go through each number and double it, so we can have a new RDD with all the numbers doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c3ebbb-5adf-4ab7-9075-5363707a9c15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create the data RDD\n",
    "dataRDD = sc.parallelize(range(20))\n",
    "\n",
    "#Define my new function (this function will not work without data) that doubles every value:\n",
    "def broken_function(x):\n",
    "  doubledDataList = []\n",
    "  #iterate through all data\n",
    "  for i in x:\n",
    "    doubledDataList.append(i * 2)\n",
    "  return doubledDataList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229f6d30-c6bc-4d24-9de2-3f1b5106f628",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First let's test our function on some pythonData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510c9413-1489-4121-9b89-0c6725d31ed1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]"
     ]
    }
   ],
   "source": [
    "# apply our function to a list of the number 0 to 19\n",
    "broken_function(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5767f0d-94bc-40af-8a46-6e68f8445c63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It works perfectly, now lets use the map function to apply it to our RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde0b0e8-9a4d-4122-8696-8c9fbae68045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create an RDD with the data values 0 to 19\n",
    "dataRDD = sc.parallelize(range(20))\n",
    "\n",
    "# use the map transformation to apply our function\n",
    "doubledDataRDD = dataRDD.map(lambda x: broken_function(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd0caf6-92df-44fa-9cf5-41776a2157f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The cell ran without any errors, so all looks good right? Unfortunately not, remember spark hasn't done any calculation yet. Let's see what happens when we try to get the data out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1423377c-894f-45a2-9c8e-ad788949dc7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-734436448619104>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdoubledDataRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/rdd.py:1825\u001B[0m, in \u001B[0;36mRDD.collect\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1823\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext):\n",
       "\u001B[1;32m   1824\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 1825\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1826\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.0 failed 1 times, most recent failure: Lost task 1.0 in stage 8.0 (TID 44) (ip-10-172-207-112.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not iterable', from <command-734436448619098>, line 8. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File \"/databricks/spark/python/pyspark/worker.py\", line 1037, in main\n",
       "    process()\n",
       "  File \"/databricks/spark/python/pyspark/worker.py\", line 1029, in process\n",
       "    serializer.dump_stream(out_iter, outfile)\n",
       "  File \"/databricks/spark/python/pyspark/serializers.py\", line 325, in dump_stream\n",
       "    vs = list(itertools.islice(iterator, batch))\n",
       "  File \"/databricks/spark/python/pyspark/util.py\", line 84, in wrapper\n",
       "    return f(*args, **kwargs)\n",
       "  File \"<command-734436448619102>\", line 5, in <lambda>\n",
       "  File \"<command-734436448619098>\", line 8, in broken_function\n",
       "TypeError: 'int' object is not iterable\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:685)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:894)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:876)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:638)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
       "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
       "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
       "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
       "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
       "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
       "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
       "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1075)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3401)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3332)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3321)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3321)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1438)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1438)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1438)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3614)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3552)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3540)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1187)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1175)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2750)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1073)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:448)\n",
       "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1071)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:282)\n",
       "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not iterable', from <command-734436448619098>, line 8. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File \"/databricks/spark/python/pyspark/worker.py\", line 1037, in main\n",
       "    process()\n",
       "  File \"/databricks/spark/python/pyspark/worker.py\", line 1029, in process\n",
       "    serializer.dump_stream(out_iter, outfile)\n",
       "  File \"/databricks/spark/python/pyspark/serializers.py\", line 325, in dump_stream\n",
       "    vs = list(itertools.islice(iterator, batch))\n",
       "  File \"/databricks/spark/python/pyspark/util.py\", line 84, in wrapper\n",
       "    return f(*args, **kwargs)\n",
       "  File \"<command-734436448619102>\", line 5, in <lambda>\n",
       "  File \"<command-734436448619098>\", line 8, in broken_function\n",
       "TypeError: 'int' object is not iterable\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:685)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:894)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:876)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:638)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
       "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
       "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
       "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
       "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
       "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
       "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
       "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1075)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-734436448619104>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdoubledDataRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/rdd.py:1825\u001B[0m, in \u001B[0;36mRDD.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1823\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext):\n\u001B[1;32m   1824\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1825\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1826\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.0 failed 1 times, most recent failure: Lost task 1.0 in stage 8.0 (TID 44) (ip-10-172-207-112.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not iterable', from <command-734436448619098>, line 8. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1037, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1029, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 325, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 84, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-734436448619102>\", line 5, in <lambda>\n  File \"<command-734436448619098>\", line 8, in broken_function\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:894)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:876)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:638)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1075)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3401)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3332)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3321)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3321)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1438)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3614)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3552)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3540)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1187)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1175)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2750)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1073)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:448)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1071)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:282)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not iterable', from <command-734436448619098>, line 8. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1037, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1029, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 325, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 84, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-734436448619102>\", line 5, in <lambda>\n  File \"<command-734436448619098>\", line 8, in broken_function\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:894)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:876)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:638)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1075)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.0 failed 1 times, most recent failure: Lost task 1.0 in stage 8.0 (TID 44) (ip-10-172-207-112.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not iterable', from <command-734436448619098>, line 8. Full traceback below:",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "doubledDataRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5474ecb1-d975-43e7-b28c-97e23218c596",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ok now we have an error, so firstly this is something we need to be careful of in spark, just because the code for our transformations runs does not mean it is correct, we only find this out when we try to get back the results.\n",
    "\n",
    "*Any idea what is wrong with the function?*\n",
    "\n",
    "The problem is that it is trying to iterate through all the data items. But `map` applies the function to each individual record in the data. So when I apply `map` it takes the first record, which is the value 0, and applies the function to it. The function then tries to iterate through the value 0 which does not work. Run the cell below which tries to apply the function to the value of 0, this is what the map transformation is doing when we use it with the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da2eabda-621d-4a1d-aec7-45710aa5129a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-734436448619106>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mbroken_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m<command-734436448619098>:8\u001B[0m, in \u001B[0;36mbroken_function\u001B[0;34m(x)\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m doubledDataList \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#iterate through all data\u001B[39;00m\n",
       "\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m x:\n",
       "\u001B[1;32m      9\u001B[0m   doubledDataList\u001B[38;5;241m.\u001B[39mappend(i \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doubledDataList\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: 'int' object is not iterable"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-734436448619106>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbroken_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m<command-734436448619098>:8\u001B[0m, in \u001B[0;36mbroken_function\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m      6\u001B[0m doubledDataList \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#iterate through all data\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m x:\n\u001B[1;32m      9\u001B[0m   doubledDataList\u001B[38;5;241m.\u001B[39mappend(i \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doubledDataList\n\n\u001B[0;31mTypeError\u001B[0m: 'int' object is not iterable",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: 'int' object is not iterable",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "broken_function(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e754b31-b108-465f-8bb9-b96736ca43da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at how to do this properly, we wrote a function that doubles an individual data item, not all the items together. Then apply it with `map` which applies a function to the data individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8408c1e0-1c41-4634-a5b8-84d86931a0ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]"
     ]
    }
   ],
   "source": [
    "# we are going to work with our RDD which contains the values of 0 to 19\n",
    "\n",
    "# here I define the function I will use\n",
    "def double_a_number(x):\n",
    "  return 2 * x\n",
    "\n",
    "# now I apply the function with map\n",
    "correct_doubled_RDD = dataRDD.map(lambda x: double_a_number(x))\n",
    "\n",
    "# check the values\n",
    "correct_doubled_RDD.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5858cb-3f35-4edf-928f-f6d9dc29e440",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It worked! Hopefully, you can see the difference between the function that didn't work (because it was written to take all of the data as an input) and the one that did (because it was written to take just one item of data as an input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8971dc75-39b6-4a5a-9d76-cfc27b7dd3cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Let's practice your understanding with an exercise\n",
    "\n",
    "First create a python variable with a list of the numbers 0 to 49, you can name the variable whatever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c0da5a2-5839-4d29-bf3d-879797408f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create your variable here\n",
    "myList = range(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a5ac65-b201-4286-a64c-263f70450420",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now load the data from your variable into an RDD (again name the new RDD whatever you like) using the *spark context* 'sc' and the parallelize function as we have done several times above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb53c2e-2d45-44e3-8f0a-94832b77167b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create your RDD here\n",
    "myRDD = sc.parallelize(myList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46bddb7d-67a7-4c83-b7c8-fb3a36c6444b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write a function that will return the square of any number. Call this function whatever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915e3ba5-2280-4121-8305-03b4e9ff8c8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#md write your function here \n",
    "def square(x):\n",
    "  return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5504084f-c804-4909-99a8-f5f4e6552e28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use `map` to apply your function to the data in the RDD creating a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4a91b0-2605-4273-a691-6f2a853a0621",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create your new RDD with map here\n",
    "newRDD = myRDD.map(square)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b2f0cf-a52c-4acc-8a6c-29ae3358f8c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use an action to return your results and make sure they are correct, the results should look something like this.  \n",
    "Out[]: [0,\n",
    " 1,\n",
    " 4,\n",
    " 9,\n",
    " 16,\n",
    " 25,\n",
    " 36,\n",
    " 49,\n",
    " 64,\n",
    " 81,\n",
    " 100,\n",
    " 121,\n",
    " 144,\n",
    " 169,\n",
    " 196,\n",
    " 225,\n",
    " 256,\n",
    " 289,\n",
    " 324,\n",
    " 361,\n",
    " 400,\n",
    " 441,\n",
    " 484,\n",
    " 529,\n",
    " 576,\n",
    " 625,\n",
    " 676,\n",
    " 729,\n",
    " 784,\n",
    " 841,\n",
    " 900,\n",
    " 961,\n",
    " 1024,\n",
    " 1089,\n",
    " 1156,\n",
    " 1225,\n",
    " 1296,\n",
    " 1369,\n",
    " 1444,\n",
    " 1521,\n",
    " 1600,\n",
    " 1681,\n",
    " 1764,\n",
    " 1849,\n",
    " 1936,\n",
    " 2025,\n",
    " 2116,\n",
    " 2209,\n",
    " 2304,\n",
    " 2401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13bdeb3-513c-4172-a88d-930445c6f832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: [0,\n 1,\n 4,\n 9,\n 16,\n 25,\n 36,\n 49,\n 64,\n 81,\n 100,\n 121,\n 144,\n 169,\n 196,\n 225,\n 256,\n 289,\n 324,\n 361,\n 400,\n 441,\n 484,\n 529,\n 576,\n 625,\n 676,\n 729,\n 784,\n 841,\n 900,\n 961,\n 1024,\n 1089,\n 1156,\n 1225,\n 1296,\n 1369,\n 1444,\n 1521,\n 1600,\n 1681,\n 1764,\n 1849,\n 1936,\n 2025,\n 2116,\n 2209,\n 2304,\n 2401]"
     ]
    }
   ],
   "source": [
    "# use an action here to return the results from you RDD\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aed7f68-9452-4469-ba76-8dbb57f49e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use the filter transformation to filter out results in your RDD (your RDD where all values have been squared) that are less than 1500 and use an action to return the results. The results should look something like this:  \n",
    "Out[]: [1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe3e842-9009-420c-9d0f-f7d54541a2b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: [1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401]"
     ]
    }
   ],
   "source": [
    "# create a new RDD with filtered results and return the values\n",
    "newRDD.filter(lambda x: x > 1500).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a047d0-f24f-44d5-997d-370fe0ba0b9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parallelization example\n",
    "\n",
    "**NOTE:** Unfortunately, this section will not produce different results for different degrees of parallelization in the community edition of Databricks. Basically, as we are only allocated in one cluster we cannot parallelize our data. However, I leave this exercise in the lab as it is important to see how this should work.\n",
    "\n",
    "Let’s look at another example, a simple method to estimate the value of Pi. This method randomly selects points in a square, then checks if those points are inside a circle within the square. The proportion inside a circle should be equal to the area of that circle, from which we can calculate Pi.\n",
    "\n",
    "Watch this short video which demonstrates what we are trying to do, estimate the value of Pi through Monte-Carlo simulation: https://www.youtube.com/watch?v=ELetCV_wX_c\n",
    "\n",
    "The `inside` function below returns true if a dart randomly thrown at the square in the diagram above lands inside the red circle and false otherwise. We use the proportion that land inside to estimate Pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d585d1a-475b-43c3-b7cf-8dde8bacab06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "numSamples =  20000000\n",
    "\n",
    "def inside(p):\n",
    "    # this function does not use the input value, it selects two random number to represent coordinates of throwing darts at a square, then determines if those darts would lie inside a circle\n",
    "    x, y = random.random(), random.random()\n",
    "    return x ** 2 + y ** 2 <= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9289300-9c94-407b-adb0-dcdadb2b0d4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we create the RDD’s, here we will specify the level of parallelization 1 for the first and 3 for the second (we specify this with the second argument to parallelize).\n",
    "\n",
    "*Note: one important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60367e6e-d298-436b-ac94-72e1bc52477e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First we create two RDD's with a bunch of records\n",
    "partition1 = sc.parallelize(range(0, numSamples), 1)\n",
    "partition2 = sc.parallelize(range(0, numSamples), 5)\n",
    "\n",
    "# Here we filter by darts that land inside a circle\n",
    "insideDart1 = partition1.filter(inside)\n",
    "insideDart2 = partition2.filter(inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36286fe-2418-4f95-a216-fe8abe5ac01b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Again this cell was quick to evaluate, as the calculation is only done when we decide to observe the results (or call another action). Let’s try the calculation with the data on one partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21e1ece-c8b3-4e99-babc-30a9e3885cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly: 3.1412664\n"
     ]
    }
   ],
   "source": [
    "pi1 =(4.0 * insideDart1.count() / numSamples)\n",
    "print(\"Pi is roughly: \" + str(pi1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb253ae-b093-4645-980b-b4cfa2843868",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's try again with the RDD we split into 2 partitions. You should see the cell below evaluates much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97699546-02cf-4e8c-a0b3-050b9eaeb114",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly: 3.1412468\n"
     ]
    }
   ],
   "source": [
    "pi1 =(4.0 * insideDart2.count() / numSamples)\n",
    "print(\"Pi is roughly: \" + str(pi1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892300a5-ffdb-4f35-a5f4-783800497b27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "NOTE: If these two commands took the same amount of time, it is because we are running on a trial cluster (as we are using the free community version of databricks) and it only has one node (so it cannot execute in parallel).\n",
    "\n",
    " We can always check the number of partitions with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f746c4-5306-4c6d-ad3f-dd3ace9f84d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: 5"
     ]
    }
   ],
   "source": [
    "insideDart2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb50321-e601-453f-97fe-690c6098e489",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the python code above we use two new RDD methods: `filter()` and `count()`. Can you tell which is a transformation and which is an action? (When did the job start execution)\n",
    "\n",
    "Spark works with functional programming where lambda (or anonymous) functions are very useful. If you are unfamiliar with these functions please review:\n",
    "\n",
    "https://www.geeksforgeeks.org/python-lambda-anonymous-functions-filter-map-reduce/\n",
    "\n",
    "It may be worth a review of the above page anyway to see examples of using lambda functions with `map` or `filter` functions. Check the example below, I use the lambda function and the map transformation to square every number in the partition1 RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef744543-7bd3-4bd2-bd33-f85059e5daf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
     ]
    }
   ],
   "source": [
    "squaredRDD = partition1.map(lambda x: x*x)\n",
    "squaredRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298ad7d2-1823-4a3b-8c76-33b352957150",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Lambda functions are a very common method to work with RDDs, especially at the ETL stage of data analysis. Write the code to create a new RDD called evenNumbers that contains only even values from the existing RDD partition1 (without using a lambda function). You can do this similarly to how we have worked above, define a new function that determines if a number is odd or even, then apply your function to the data using the filter function. Check how in the example above we are using the filter function to go from an RDD with all sampled random numbers, to an RDD with just samples that were inside the circle. For more information you can also check this explanation of the filter function in spark:\n",
    "\n",
    "https://backtobazics.com/big-data/spark/apache-spark-filter-example/\n",
    "\n",
    "You can check the first 10 values with the take() function ie. evenNumbers.take(10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12a8f5c-2e1f-443d-88c0-4baed5b58f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#define a function that returns true if the input is an even number (otherwise false)\n",
    "def even(x):\n",
    "  return (x % 2) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03954a62-1ff0-48a5-bfb6-4c4a0b1df56c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply the function using filter to the partition1 RDD to create a new RDD\n",
    "myResult = partition1.filter(even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6309faa6-9813-4707-ba36-d54e5bb4996d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
     ]
    }
   ],
   "source": [
    "# Check your results by using the take action to take the top 10 values from the result and check they are all even\n",
    "myResult.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ad21b32-88ec-4e84-9173-12752614ff3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use a lamda function to achieve the same result (Hint: the filter function can be helpful here again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b469af2d-51f0-409f-a614-57ddbbcece4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
     ]
    }
   ],
   "source": [
    "partition1.filter(lambda x: (x % 2) == 0).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd8b6ce-51a9-4303-915f-115e8ea579a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Paired RDD\n",
    "\n",
    "Spark Paired RDDs are nothing but RDDs containing a lot of key-value pairs. Basically, key-value pairs (KVP) consists of two linked data items. In pyspark we use tuples to store the linked key and value like this (key, value).  Here, the key is the identifier and the value is the data corresponding to the key. Spark operations work on RDDs containing any type of objects, however, key-value pair RDDs have a few extra operations. Such as:\n",
    "\n",
    "groupByKey - groups the values of the RDD by key.\n",
    "\n",
    "reduceByKey - performs aggregation on the grouped values corresponding to a key.\n",
    "\n",
    "mapValues - transformation applies a function to each of the values of the pair RDD without changing the key.\n",
    "\n",
    "We will see examples below with a dataset of sales, where the key is the country of sale and the value of the amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4822e9a-2331-442f-81c8-f1ca503a1eff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here we create the data and load into a RDD\n",
    "data = [(\"Portugal\",30.94),(\"Spain\",61.43),(\"France\",50.37),(\"Germany\",67.51),(\"Germany\",57.12),(\"Portugal\",20.12),(\"Germany\",76.92),(\"Portugal\",32.53),(\"Spain\",30.39),(\"Germany\",21.11),(\"Spain\",56.98),(\"France\",64.99),(\"France\",39.2),(\"Germany\",27.99),(\"Portugal\",60.59),(\"Portugal\",60.58),(\"Germany\",87.11),(\"Germany\",77.25),(\"Germany\",40.74),(\"Portugal\",66.19),(\"France\",44.47)]\n",
    "paired_RDD = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9b4d9e-586a-45f4-99ff-7f95aa22383d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The reduceByKey function works only for RDDs which contain key and value pairs of elements (i.e. RDDs having tuple as a data element). It is a transformation operation which means it is lazily evaluated. We need to pass one associative function as a parameter, which will be applied to the source RDD and will create a new RDD as with resulting values(i.e. key value pair).\n",
    "\n",
    "The associative function accepts two arguments and returns a single element. It performs merging on the clusters remotely then locally using reduce function and then sends records across the partitions for preparing the final results.\n",
    "\n",
    "Below we use a lambda function that takes two inputs (x and y) and sums them together. This returns the sum of sales by country in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4a9670-fb45-4be4-80fa-ddd0bbd61eef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: [('Spain', 148.79999999999998),\n ('Germany', 455.75000000000006),\n ('Portugal', 270.95),\n ('France', 199.03)]"
     ]
    }
   ],
   "source": [
    "sumRDD = paired_RDD.reduceByKey(lambda x,y: x + y)\n",
    "sumRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c990502b-275f-44ef-a787-709522984d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As the name suggests, the groupByKey function in Apache Spark just groups all values with respect to a single key. Unlike reduceByKey it doesn’t perform any operation on the final output. It just groups the data and returns in the form of an iterator. Take a look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75bd9ba3-4a30-4697-8b99-9ad7c1be61f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain\n61.43\n30.39\n56.98\n"
     ]
    }
   ],
   "source": [
    "groupRDD = paired_RDD.groupByKey()\n",
    "\n",
    "#What is the key for the first group\n",
    "print(groupRDD.collect()[0][0])\n",
    "\n",
    "#What are the values in the first group\n",
    "for i in groupRDD.collect()[0][1]:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d21afc8-87e9-4e63-935e-8debb7c6cc86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When we use map() with a Pair RDD, we get access to both the key & value. Often we are only interested in accessing the value (and not key). In those cases, we can use mapValues() instead of map() to apply a function. Here I use the 'sorted' function and apply it to the grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71450d75-621b-41d7-819e-b030a4266488",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain\n30.39\n56.98\n61.43\n"
     ]
    }
   ],
   "source": [
    "sortedRDD = groupRDD.mapValues(lambda x: sorted(x))\n",
    "\n",
    "#What is the key for the first group\n",
    "print(sortedRDD.collect()[0][0])\n",
    "\n",
    "#What are the values in the first group\n",
    "for i in sortedRDD.collect()[0][1]:\n",
    "  print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d12731-3554-4e53-ba34-a70dbc6ad86f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Use mapValues on the paired_RDD to round values to the nearest euro in a new RDD. Python has a 'round' function which can be useful for you here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6982b83-6836-4813-bf58-06684a9121f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[41]: [('Portugal', 31), ('Spain', 61), ('France', 50)]"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "paired_RDD.mapValues(lambda x:  round(x)).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a384c0-7def-4958-88bd-8bccf92047a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use reduce to find the highest sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4513ac1-abbc-46f4-982d-4ff57bfee47f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: ('Germany', 87.11)"
     ]
    }
   ],
   "source": [
    "# Extra explanation below\n",
    "paired_RDD.reduce(lambda x,y: x if (x[1] > y[1]) else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b41f14-1a34-441d-ac0f-40a896538618",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use reduceByKey to find the highest sale by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2e9513-9285-4a77-b0b3-1f02649138c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: [('Spain', 61.43), ('Germany', 87.11), ('Portugal', 66.19), ('France', 64.99)]"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "paired_RDD.reduceByKey(lambda x,y: x if (x > y) else y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4eab08a-9c70-4ccc-9f74-4aec9c4d41b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Do you notice the difference when fetching the results?  \n",
    "If you look at the cheatsheet provided you can see one is a transformation and the other is and action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f75ff1-076d-4123-8db2-980f53549a6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Extra section about the reduce method  \n",
    "\n",
    "Reduce or also refered as [fold](https://en.wikipedia.org/wiki/Fold_(higher-order_function) operation is an important concept in spark that might cause confusion when coming to functional programming environments like spark.\n",
    "Below it's an example of a possible implementation of the reduce method in pure pyhton  \n",
    "\n",
    "``` python\n",
    "def reduce(function, iterable):\n",
    "    # Transform the list into an iterator\n",
    "    it = iter(iterable)\n",
    "    # Take the first value of the list\n",
    "    value = next(it)\n",
    "    \n",
    "    #Iterate over the rest list applying the function to each element of the list and the past result  \n",
    "    for element in it:\n",
    "        value = function(value, element)\n",
    "    return value\n",
    "```\n",
    "\n",
    "And below is the previous exercise but in pure python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b00033-290f-415b-8760-c5965c5a9f90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Portugal', 30.94) or ('Spain', 61.43) -> ('Spain', 61.43)\n('Spain', 61.43) or ('France', 50.37) -> ('Spain', 61.43)\n('Spain', 61.43) or ('Germany', 67.51) -> ('Germany', 67.51)\n('Germany', 67.51) or ('Germany', 57.12) -> ('Germany', 67.51)\n('Germany', 67.51) or ('Portugal', 20.12) -> ('Germany', 67.51)\n('Germany', 67.51) or ('Germany', 76.92) -> ('Germany', 76.92)\n('Germany', 76.92) or ('Portugal', 32.53) -> ('Germany', 76.92)\n('Germany', 76.92) or ('Spain', 30.39) -> ('Germany', 76.92)\n('Germany', 76.92) or ('Germany', 21.11) -> ('Germany', 76.92)\n('Germany', 76.92) or ('Spain', 56.98) -> ('Germany', 76.92)\n('Germany', 76.92) or ('France', 64.99) -> ('Germany', 76.92)\n('Germany', 76.92) or ('France', 39.2) -> ('Germany', 76.92)\n('Germany', 76.92) or ('Germany', 27.99) -> ('Germany', 76.92)\n('Germany', 76.92) or ('Portugal', 60.59) -> ('Germany', 76.92)\n('Germany', 76.92) or ('Portugal', 60.58) -> ('Germany', 76.92)\n('Germany', 76.92) or ('Germany', 87.11) -> ('Germany', 87.11)\n('Germany', 87.11) or ('Germany', 77.25) -> ('Germany', 87.11)\n('Germany', 87.11) or ('Germany', 40.74) -> ('Germany', 87.11)\n('Germany', 87.11) or ('Portugal', 66.19) -> ('Germany', 87.11)\n('Germany', 87.11) or ('France', 44.47) -> ('Germany', 87.11)\nThe highest is ('Germany', 87.11)\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "data = [\n",
    "  (\"Portugal\",30.94),(\"Spain\",61.43),(\"France\",50.37),(\"Germany\",67.51),\n",
    "  (\"Germany\",57.12),(\"Portugal\",20.12),(\"Germany\",76.92),(\"Portugal\",32.53),\n",
    "  (\"Spain\",30.39),(\"Germany\",21.11),(\"Spain\",56.98),(\"France\",64.99),\n",
    "  (\"France\",39.2),(\"Germany\",27.99),(\"Portugal\",60.59),(\"Portugal\",60.58),\n",
    "  (\"Germany\",87.11),(\"Germany\",77.25),(\"Germany\",40.74),(\"Portugal\",66.19),\n",
    "  (\"France\",44.47)\n",
    "]\n",
    "\n",
    "def tuple_greater_than(x, y):\n",
    "  if x[1] > y[1]:\n",
    "    print(f\"{x} or {y} -> {x}\")\n",
    "    return x\n",
    "  else:Tome nota\n",
    "    print(f\"{x} or {y} -> {y}\")\n",
    "    return y\n",
    "\n",
    "highest = reduce(tuple_greater_than, data)\n",
    "print(f\"The highest is {highest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b10a315a-e8e3-45c5-96ef-b2d88c5e9812",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Examples inspired from https://realpython.com/python-reduce-function/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab4_bda_rdd_solutus",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
