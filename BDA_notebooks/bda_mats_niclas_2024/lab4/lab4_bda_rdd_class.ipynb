{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfbc0c36-34b9-4502-9c70-1193537d5789",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# BD Lab - Spark practice\n",
    "\n",
    "Apache Spark is a lightning-fast cluster computing technology, designed for fast computation. It is based on Hadoop MapReduce and it extends the MapReduce model to efficiently use it for more types of computations, which includes interactive queries and stream processing. The main feature of Spark is its in-memory cluster computing that increases the processing speed of an application.\n",
    "\n",
    "Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries and streaming. Apart from supporting all these workload in a respective system, it reduces the management burden of maintaining separate tools.\n",
    "\n",
    "Apache Spark has the following features.\n",
    "\n",
    "Speed − Spark helps to run an application on a Hadoop cluster up to 100 times faster than in-memory, and 10 times faster when running on disk. This is possible by reducing the number of read/write operations to disk. It stores the intermediate processing data in memory.\n",
    "\n",
    "Supports multiple languages − Spark provides built-in APIs in Java, Scala, or Python. Therefore, you can write applications in different languages. Spark comes up with 80 high-level operators for interactive querying.\n",
    "\n",
    "Advanced Analytics − Spark not only supports ‘Map’ and ‘reduce’. It also supports SQL queries, Streaming data, Machine learning (ML), and Graph algorithms.\n",
    "\n",
    "## Background Spark Context\n",
    "\n",
    "There are a few spark concepts that we will now introduce before we get started. The sparkcontext object which allows us to interact with spark and the spark data structure of RDD.\n",
    "Sparkcontext is basically just an entry point to any Spark functionality. Spark applications are run as independent sets of processes, coordinated by a Spark Context in a driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ec1aeb-f571-486d-861b-4ce3947aeae5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Title](https://annefou.github.io/pyspark/slides/images/SparkRuntime.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1270ef1a-8e30-4d53-9f97-e532ba108631",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check the Spark version you are running and the Databricks version\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d7dc74-d89c-451b-819e-cc64720df6ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The spark context may be automatically created (for instance if you call pyspark from the shells (the Spark context is then called sc).\n",
    "\n",
    "You will see we will use the sparkcontext by invoking methods on the sc object to interact with spark.\n",
    "\n",
    "QUICK NOTE: The spark context is actually replaced by the spark session in later versions of spark. We will talk about this in the following labs, but using the spark context now will mean you have familiarity with both.\n",
    "\n",
    "## Background on RDD\n",
    "\n",
    "We will start by looking at the spark concept of Resilient Distributed Datasets (RDD). RDD is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
    "\n",
    "Formally, an RDD is a <b> read-only, partitioned </b> collection of records. RDDs can be created through deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "\n",
    "There are two ways to create RDDs − parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.\n",
    "\n",
    "You can apply multiple operations on these RDDs to achieve a certain task. To apply operations on these RDD's, there are two ways −\n",
    "\n",
    "<b>Transformation</b> − These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations.\n",
    "\n",
    "<b>Action</b> − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver.\n",
    "\n",
    "<b>Laziness of Spark transformations </b>\n",
    "It’s important to understand that transformations are evaluated lazily, meaning computation doesn’t take place until you invoke an action. Once an action is triggered on an RDD, Spark examines the RDD’s lineage and uses that information to build a “graph of operations” that needs to be executed in order to compute the action. Think of a transformation as a sort of diagram that tells Spark which operations need to happen and in which order once an action gets executed. We will work with examples that demonstrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24efeeab-08d2-432b-9df6-909b38783614",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark - Lazy evaluation\n",
    "\n",
    "Apache Spark is written in Scala programming language. To support Python with Spark, Apache Spark Community released a tool, PySpark. Using PySpark, you can work with RDDs in Python programming language also.\n",
    "\n",
    "PySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context. The majority of data scientists and analytics experts today use Python because of its rich library set. Integrating Python with Spark is a boon to them.\n",
    "\n",
    "## Lazy evaluation and caching\n",
    "\n",
    "In this example we will take a quick look at what lazy execution means for our code and how caching works. Take a look at the function we will use below (and run the cell). As you can see it is designed to take time to evaluate, this function just waits one second before continuing. Basically, we are representing a task we want to do that takes time.\n",
    "\n",
    "### Cache\n",
    "\n",
    "- Reading data from source(hdfs:// or s3://) is time consuming. So after you read data from the source and apply all the common operations, cache it if you are going to reuse the data.\n",
    "- By caching you create a checkpoint in your spark application and if further down the execution of application any of the tasks fail your application will be able to recompute the lost RDD partition from the cache.\n",
    "- If you don’t have enough memory data will be cached at the local disk of executor which will also be faster than reading from the source.\n",
    "- If you can only cache a fraction of data it will also improve the performance, the rest of the data can be recomputed by spark and that’s what resilient in RDD means.\n",
    "\n",
    "More: https://towardsdatascience.com/apache-spark-caching-603154173c48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4adecce5-438b-4cb7-ad27-e4782ecbe2ff",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create a fucntion called slowLoad that we will use later\n",
    "def slowLoad(x):\n",
    "    time.sleep(1) # sleeps for 1 second\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c37257-e040-4148-aad3-baf51e01048c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's define a quick dataset to work with, first we create data in Python. Then we will create an RDD based on this input. To do so we use the spark context object to parallelize the data. To return the data we use the collect() action which evaluates the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "622c437a-2c1e-44d1-8d21-bb3e6206378e",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create Python data (a list of the numbers 0 to 19)\n",
    "dataPython = range(0,20)\n",
    "\n",
    "#load the data into an RDD by spreading it out into our big data cluster\n",
    "smallRDD = sc.parallelize(dataPython,1)\n",
    "\n",
    "#return the data from the RDD using an 'action' called collect\n",
    "smallRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65ea3657-b5b3-4b4c-9a7b-7296e0c3e82a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we create a new RDD that uses our function. We use a map function (this is a transformation, we will talk more about this next), as with the map reduce framework we started with. Map allows us to apply any function to all our data. Here we are simulating a data transformation that takes a long time. For every item in our database, the function will take 1 second to evaluate (approximately). So with 20 items in our dataset applying the function 20 times should take 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c997eb90-4044-4a03-8b45-844290ba91fe",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use the map transformation to apply the function we made to the data\n",
    "newRDD = smallRDD.map(slowLoad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e7c3597-ea91-494a-9293-006780ac3558",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That cell probably evaluated quicker than you expected. In my case it took 0.04 seconds but the function we created should take 1 second for each item in our database with 20 items.\n",
    "\n",
    "What do you think is currently in the new RDD we have just created that is called newRDD?\n",
    "\n",
    "Right now nothing (or at least no data), but as soon as we try to look at the contents (or otherwise use the data) spark will need to perform the computation to create the new RDD. This is the difference between transformations (such as map) and actions (such as collect). So let’s apply the collect action to see what is in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b395f714-e1e1-4818-9644-e173d8acf09c",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the collect action to resturn the contents of our data\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d43c820b-aebc-4af7-8356-3334bdd43191",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You should see that only now was our function actually being used (as it takes time), not when we created the newRDD, as only now was an action used.\n",
    "\n",
    "Hopefully you can see this is completely different to running cells in Python normally.\n",
    "\n",
    "Run the above cell **TWICE** to check that it takes the same amount of time (that in both cases our data transformation is being recalculated).\n",
    "\n",
    "So a couple of key take-aways. We use transformations to apply functions to our data, for example we could have a function that will clean the data and use map to apply it. When we use the transformation we get a new RDD that has had the function applied to all of the data, so in our data cleaning example we would get a new RDD with clean data. **BUT** the new RDD doesn't actually contain any data, just the instructions for how to create the cleaned data. We can apply many transformations and no actual new data will have been created. But when we want to see the results of course the instructions to change the data will need to be executed, so when we perform an 'action', such as collect, which will return a value all the transformations will be applied.\n",
    "\n",
    "Often we will want to work with transformed data in multiple queries, for example we might want to use our cleaned data multiple times without having to redo the calculations to clean the data multiple times. Or alternatively, we might want to use an iterative algorithm (such as training machine learning) and we don't want to have to redo all previous transformations in each iteration. In these cases we can 'persist' the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f317cd70-7f52-406d-a4f9-5660e02ce7ad",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tell the RDD to store the results, not just the instructions for creating the results, for future use the cache function.\n",
    "newRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0bb966c-932f-4985-be78-e3b0f143b1a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the below cell **THREE TIMES** to see the difference. The first time should take 20 seconds again. But after the first time the data the data is calculated it will be held in memory so that it can be quickly retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe6ca2e5-e85c-4128-a308-19a6a5c9d2da",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the collect action to get the contents of the RDD\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f8b2ab-530f-4c47-87b8-f94430106ec1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the above case we have persisted the RDD to memory (RAM) we can also persist to disk (or a combination of the two). See https://data-flair.training/blogs/apache-spark-rdd-persistence-caching/ for more information.\n",
    "\n",
    "In the above example we have been working with the basic data structure of spark: the RDD. You have seen how to create an RDD by parallelizing some existing python data and you have seen how to get results out by using an **action** 'collect' which returns the values from an RDD. There is another important operation we will be using with RDDs and that is the **transformation** which creates a new RDD from an existing one, usually with some change we have made to the data (a transformation of the data). For example we map load out data into an RDD, then decide to clean the data by applying a transformation and getting a new RDD with clean data. Remember, RDDs are immutable, this means they cannot be changed, so we can't just clean the data inside the RDD we loaded the data into, as this would be changing the contents of the RDD which is not possible, we need to work with transformations. Two of the most common transformations are explained below.\n",
    "\n",
    "First we start by creating some data (a list with three strings) and parallelizing it into our cluster to create an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab0d7062-40b5-465c-a3e8-5f7856ba539e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the data as a list in Python\n",
    "mylist = [\"This is the first line\", \"Now the second line\", \"Finally the third line\", \"The Hittite capital of Hattusa lies in modern-day Turkey\"]\n",
    "\n",
    "# Load the data into spark to create an RDD with the data inside\n",
    "myRDD = sc.parallelize(mylist)\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4272afb7-0e50-4fdc-a6ba-768577b3a510",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Take a look at the cell below, here I apply the map transformation to our RDD, the output of which is a new RDD which I have called 'splitListRDD'. The map transformation applies a function to all of the data in the RDD, here I have created a lambda function which will split the strings. To see what is inside the new RDD we then need to use the collect action to get the data out. Remember, that we have lazy execution so that when I applied the map transformation nothing actually happened, spark just keeps a record of \"how to create\" the data we want, it is only when we perform an action that any calculation is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0c99e71-1c52-4402-8a3a-2374fc63d0e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#apply a funtion, in this case a lambda function that splits the words, with map to create a new RDD\n",
    "splitListRDD = myRDD.map(lambda line: line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65003c79-2079-42e9-a34a-6f1c3e470dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#return the results from the new RDD\n",
    "splitListRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c0282ba-0286-42cb-86de-90b5f8406c35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "So we see we have transformed our data, which was a list of three strings, into **a list of three lists: inside each of these three lists are the words split separately.** It is important to see that the number of items in the data set is the same, before we had three strings (three sentences), now we have three lists (lists of each word in the original sentence).\n",
    "\n",
    "This is important as it is the difference to the other most common transformation we will be using called flatMap. Like  map, flatMap will apply a function to all out data in the database, however, this function 'flattens' out all the data. So imagine we wanted all the words from our data, maybe to count them, but our previous transformation didn't quite help as we ended up with words all in different lists. The flatmap transformation is the answer in that it removes this structure (flattens out the data so it is all on the same level of the list). Take a look at the below example and see the difference in the result returned, now we have all our words together in a single list. Also notice how I 'chain' together the transformation and the action into one line, this is a common way to apply transformations all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1508b60c-ac7e-4bfc-8a38-5d80eab6ac90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#apply a funtion, in this case a lambda function that list the words, with flatmap and return the results\n",
    "myRDD.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56bb902-f957-4c2f-8a6c-6ae5ffefe46a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's worth noting flatMap just removes one level of the data, take a look at the example below with lots of lists inside lists, flatmap has only removed the structure at one level. Take a look at 8 which was inside a list inside the main data list. After the flatmap 8 is just inside the data list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8ed0a6d-af39-4761-b57c-b5d06c86d1d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create the data\n",
    "dataTest = [[0,[1, 2, 3, 4, 5],7], [8], [9, [10,[11, 12]]]]\n",
    "\n",
    "#load the data into an RDD\n",
    "dataTestRDD = sc.parallelize(dataTest)\n",
    "\n",
    "#take a look at the contents of the RDD\n",
    "dataTestRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69b2ffb1-9fe2-4d80-9040-1341984a3e38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This line of code applies that flatmap and returns the values\n",
    "dataTestRDD.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f45f04-fefb-4b84-9dfd-3ff89fd2ded2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another popular transformation is filter that reduces the amount of data in the RDD to what we are interested in. For example in the code below I use filter and the lambda function to remove all numbers less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ef57231-2b46-4370-8111-0dbf44f325d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create the data and load into an RDD\n",
    "numberRDD = sc.parallelize([1,2,3,4,-1,-2,5,-5,6,0])\n",
    "\n",
    "# filter the data using the filter transformation, this creates a new RDD with the filtered data\n",
    "filteredRDD = numberRDD.filter(lambda x: x>=0)\n",
    "\n",
    "# return the contents of the filtered data\n",
    "filteredRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81ad8acd-8cb0-45fb-8f3a-a8110978cae0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's look at a common mistake that people can make when using a map transfromation for the first time. I'm going to use a dataset with the number 0 to 19, and I'm going to create a function that will go through each number and double it, so we can have a new RDD with all the numbers doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5c3ebbb-5adf-4ab7-9075-5363707a9c15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create the data RDD\n",
    "dataRDD = sc.parallelize(range(20))\n",
    "\n",
    "#Define my new function (this function will not work without data) that doubles every value:\n",
    "def broken_function(x):\n",
    "  doubledDataList = []\n",
    "  #iterate through all data\n",
    "  for i in x:\n",
    "    doubledDataList.append(i * 2)\n",
    "  return doubledDataList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "229f6d30-c6bc-4d24-9de2-3f1b5106f628",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First let's test our function on some pythonData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "510c9413-1489-4121-9b89-0c6725d31ed1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply our function to a list of the number 0 to 19\n",
    "broken_function(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5767f0d-94bc-40af-8a46-6e68f8445c63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It works perfectly, now lets use the map function to apply it to our RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fde0b0e8-9a4d-4122-8696-8c9fbae68045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an RDD with the data values 0 to 19\n",
    "dataRDD = sc.parallelize(range(20))\n",
    "\n",
    "# use the map transformation to apply our function\n",
    "doubledDataRDD = dataRDD.map(lambda x: broken_function(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccd0caf6-92df-44fa-9cf5-41776a2157f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The cell ran without any errors, so all looks good right? Unfortunately not, remember spark hasn't done any calculation yet. Let's see what happens when we try to get the data out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1423377c-894f-45a2-9c8e-ad788949dc7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "doubledDataRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5474ecb1-d975-43e7-b28c-97e23218c596",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ok now we have an error, so firstly this is something we need to be careful of in spark, just because the code for our transformations runs does not mean it is correct, we only find this out when we try to get back the results.\n",
    "\n",
    "*Any idea what is wrong with the function?*\n",
    "\n",
    "The problem is that it is trying to iterate through all the data items. But `map` applies the function to each individual record in the data. So when I apply `map` it takes the first record, which is the value 0, and applies the function to it. The function then tries to iterate through the value 0 which does not work. Run the cell below which tries to apply the function to the value of 0, this is what the map transformation is doing when we use it with the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da2eabda-621d-4a1d-aec7-45710aa5129a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "broken_function(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e754b31-b108-465f-8bb9-b96736ca43da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at how to do this properly, we wrote a function that doubles an individual data item, not all the items together. Then apply it with `map` which applies a function to the data individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8408c1e0-1c41-4634-a5b8-84d86931a0ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we are going to work with our RDD which contains the values of 0 to 19\n",
    "\n",
    "# here I define the function I will use\n",
    "def double_a_number(x):\n",
    "  return 2 * x\n",
    "\n",
    "# now I apply the function with map\n",
    "correct_doubled_RDD = dataRDD.map(lambda x: double_a_number(x))\n",
    "\n",
    "# check the values\n",
    "correct_doubled_RDD.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c5858cb-3f35-4edf-928f-f6d9dc29e440",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It worked! Hopefully, you can see the difference between the function that didn't work (because it was written to take all of the data as an input) and the one that did (because it was written to take just one item of data as an input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8971dc75-39b6-4a5a-9d76-cfc27b7dd3cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Let's practice your understanding with an exercise\n",
    "\n",
    "First create a python variable with a list of the numbers 0 to 49, you can name the variable whatever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c0da5a2-5839-4d29-bf3d-879797408f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create your variable here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a5ac65-b201-4286-a64c-263f70450420",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now load the data from your variable into an RDD (again name the new RDD whatever you like) using the *spark context* 'sc' and the parallelize function as we have done several times above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcb53c2e-2d45-44e3-8f0a-94832b77167b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create your RDD here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46bddb7d-67a7-4c83-b7c8-fb3a36c6444b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write a function that will return the square of any number. Call this function whatever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "915e3ba5-2280-4121-8305-03b4e9ff8c8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#md write your function here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5504084f-c804-4909-99a8-f5f4e6552e28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use `map` to apply your function to the data in the RDD creating a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4a91b0-2605-4273-a691-6f2a853a0621",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create your new RDD with map here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80b2f0cf-a52c-4acc-8a6c-29ae3358f8c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use an action to return your results and make sure they are correct, the results should look something like this.  \n",
    "Out[]: [0,\n",
    " 1,\n",
    " 4,\n",
    " 9,\n",
    " 16,\n",
    " 25,\n",
    " 36,\n",
    " 49,\n",
    " 64,\n",
    " 81,\n",
    " 100,\n",
    " 121,\n",
    " 144,\n",
    " 169,\n",
    " 196,\n",
    " 225,\n",
    " 256,\n",
    " 289,\n",
    " 324,\n",
    " 361,\n",
    " 400,\n",
    " 441,\n",
    " 484,\n",
    " 529,\n",
    " 576,\n",
    " 625,\n",
    " 676,\n",
    " 729,\n",
    " 784,\n",
    " 841,\n",
    " 900,\n",
    " 961,\n",
    " 1024,\n",
    " 1089,\n",
    " 1156,\n",
    " 1225,\n",
    " 1296,\n",
    " 1369,\n",
    " 1444,\n",
    " 1521,\n",
    " 1600,\n",
    " 1681,\n",
    " 1764,\n",
    " 1849,\n",
    " 1936,\n",
    " 2025,\n",
    " 2116,\n",
    " 2209,\n",
    " 2304,\n",
    " 2401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b13bdeb3-513c-4172-a88d-930445c6f832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use an action here to return the results from you RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aed7f68-9452-4469-ba76-8dbb57f49e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use the filter transformation to filter out results in your RDD (your RDD where all values have been squared) that are less than 1500 and use an action to return the results. The results should look something like this:  \n",
    "Out[]: [1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbe3e842-9009-420c-9d0f-f7d54541a2b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a new RDD with filtered results and return the values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66a047d0-f24f-44d5-997d-370fe0ba0b9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parallelization example\n",
    "\n",
    "**NOTE:** Unfortunately, this section will not produce different results for different degrees of parallelization in the community edition of Databricks. Basically, as we are only allocated in one cluster we cannot parallelize our data. However, I leave this exercise in the lab as it is important to see how this should work.\n",
    "\n",
    "Let’s look at another example, a simple method to estimate the value of Pi. This method randomly selects points in a square, then checks if those points are inside a circle within the square. The proportion inside a circle should be equal to the area of that circle, from which we can calculate Pi.\n",
    "\n",
    "Watch this short video which demonstrates what we are trying to do, estimate the value of Pi through Monte-Carlo simulation: https://www.youtube.com/watch?v=ELetCV_wX_c\n",
    "\n",
    "The `inside` function below returns true if a dart randomly thrown at the square in the diagram above lands inside the red circle and false otherwise. We use the proportion that land inside to estimate Pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d585d1a-475b-43c3-b7cf-8dde8bacab06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "numSamples =  2000000\n",
    "\n",
    "def inside(p):\n",
    "    # this function does not use the input value, it selects two random number to represent coordinates of throwing darts at a square, then determines if those darts would lie inside a circle\n",
    "    x, y = random.random(), random.random()\n",
    "    return x ** 2 + y ** 2 <= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9289300-9c94-407b-adb0-dcdadb2b0d4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we create the RDD’s, here we will specify the level of parallelization 1 for the first and 3 for the second (we specify this with the second argument to parallelize).\n",
    "\n",
    "*Note: one important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60367e6e-d298-436b-ac94-72e1bc52477e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First we create two RDD's with a bunch of records\n",
    "partition1 = sc.parallelize(range(0, numSamples), 1)\n",
    "partition2 = sc.parallelize(range(0, numSamples), 5)\n",
    "\n",
    "# Here we filter by darts that land inside a circle\n",
    "insideDart1 = partition1.filter(inside)\n",
    "insideDart2 = partition2.filter(inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d36286fe-2418-4f95-a216-fe8abe5ac01b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Again this cell was quick to evaluate, as the calculation is only done when we decide to observe the results (or call another action). Let’s try the calculation with the data on one partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f21e1ece-c8b3-4e99-babc-30a9e3885cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi1 =(4.0 * insideDart1.count() / numSamples)\n",
    "print(\"Pi is roughly: \" + str(pi1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fb253ae-b093-4645-980b-b4cfa2843868",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's try again with the RDD we split into 2 partitions. You should see the cell below evaluates much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97699546-02cf-4e8c-a0b3-050b9eaeb114",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi1 =(4.0 * insideDart2.count() / numSamples)\n",
    "print(\"Pi is roughly: \" + str(pi1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "892300a5-ffdb-4f35-a5f4-783800497b27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "NOTE: If these two commands took the same amount of time, it is because we are running on a trial cluster (as we are using the free community version of databricks) and it only has one node (so it cannot execute in parallel).\n",
    "\n",
    " We can always check the number of partitions with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f746c4-5306-4c6d-ad3f-dd3ace9f84d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "insideDart2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeb50321-e601-453f-97fe-690c6098e489",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the python code above we use two new RDD methods: `filter()` and `count()`. Can you tell which is a transformation and which is an action? (When did the job start execution)\n",
    "\n",
    "Spark works with functional programming where lambda (or anonymous) functions are very useful. If you are unfamiliar with these functions please review:\n",
    "\n",
    "https://www.geeksforgeeks.org/python-lambda-anonymous-functions-filter-map-reduce/\n",
    "\n",
    "It may be worth a review of the above page anyway to see examples of using lambda functions with `map` or `filter` functions. Check the example below, I use the lambda function and the map transformation to square every number in the partition1 RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef744543-7bd3-4bd2-bd33-f85059e5daf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "squaredRDD = partition1.map(lambda x: x*x)\n",
    "squaredRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "298ad7d2-1823-4a3b-8c76-33b352957150",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Lambda functions are a very common method to work with RDDs, especially at the ETL stage of data analysis. Write the code to create a new RDD called evenNumbers that contains only even values from the existing RDD partition1 (without using a lambda function). You can do this similarly to how we have worked above, define a new function that determines if a number is odd or even, then apply your function to the data using the filter function. Check how in the example above we are using the filter function to go from an RDD with all sampled random numbers, to an RDD with just samples that were inside the circle. For more information you can also check this explanation of the filter function in spark:\n",
    "\n",
    "https://backtobazics.com/big-data/spark/apache-spark-filter-example/\n",
    "\n",
    "You can check the first 10 values with the take() function ie. evenNumbers.take(10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f12a8f5c-2e1f-443d-88c0-4baed5b58f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define a function that returns true if the input is an even number (otherwise false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03954a62-1ff0-48a5-bfb6-4c4a0b1df56c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply the function using filter to the partition1 RDD to create a new RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6309faa6-9813-4707-ba36-d54e5bb4996d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check your results by using the take action to take the top 10 values from the result and check they are all even\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ad21b32-88ec-4e84-9173-12752614ff3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use a lamda function to achieve the same result (Hint: the filter function can be helpful here again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b469af2d-51f0-409f-a614-57ddbbcece4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd8b6ce-51a9-4303-915f-115e8ea579a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Paired RDD\n",
    "\n",
    "Spark Paired RDDs are nothing but RDDs containing a lot of key-value pairs. Basically, key-value pairs (KVP) consists of two linked data items. In pyspark we use tuples to store the linked key and value like this (key, value).  Here, the key is the identifier and the value is the data corresponding to the key. Spark operations work on RDDs containing any type of objects, however, key-value pair RDDs have a few extra operations. Such as:\n",
    "\n",
    "groupByKey - groups the values of the RDD by key.\n",
    "\n",
    "reduceByKey - performs aggregation on the grouped values corresponding to a key.\n",
    "\n",
    "mapValues - transformation applies a function to each of the values of the pair RDD without changing the key.\n",
    "\n",
    "We will see examples below with a dataset of sales, where the key is the country of sale and the value of the amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4822e9a-2331-442f-81c8-f1ca503a1eff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we create the data and load into a RDD\n",
    "data = [(\"Portugal\",30.94),(\"Spain\",61.43),(\"France\",50.37),(\"Germany\",67.51),(\"Germany\",57.12),(\"Portugal\",20.12),(\"Germany\",76.92),(\"Portugal\",32.53),(\"Spain\",30.39),(\"Germany\",21.11),(\"Spain\",56.98),(\"France\",64.99),(\"France\",39.2),(\"Germany\",27.99),(\"Portugal\",60.59),(\"Portugal\",60.58),(\"Germany\",87.11),(\"Germany\",77.25),(\"Germany\",40.74),(\"Portugal\",66.19),(\"France\",44.47)]\n",
    "paired_RDD = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf9b4d9e-586a-45f4-99ff-7f95aa22383d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The reduceByKey function works only for RDDs which contain key and value pairs of elements (i.e. RDDs having tuple as a data element). It is a transformation operation which means it is lazily evaluated. We need to pass one associative function as a parameter, which will be applied to the source RDD and will create a new RDD as with resulting values(i.e. key value pair).\n",
    "\n",
    "The associative function accepts two arguments and returns a single element. It performs merging on the clusters remotely then locally using reduce function and then sends records across the partitions for preparing the final results.\n",
    "\n",
    "Below we use a lambda function that takes two inputs (x and y) and sums them together. This returns the sum of sales by country in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e4a9670-fb45-4be4-80fa-ddd0bbd61eef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sumRDD = paired_RDD.reduceByKey(lambda x,y: x + y)\n",
    "sumRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c990502b-275f-44ef-a787-709522984d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As the name suggests, the groupByKey function in Apache Spark just groups all values with respect to a single key. Unlike reduceByKey it doesn’t perform any operation on the final output. It just groups the data and returns in the form of an iterator. Take a look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75bd9ba3-4a30-4697-8b99-9ad7c1be61f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groupRDD = paired_RDD.groupByKey()\n",
    "\n",
    "#What is the key for the first group\n",
    "print(groupRDD.collect()[0][0])\n",
    "\n",
    "#What are the values in the first group\n",
    "for i in groupRDD.collect()[0][1]:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d21afc8-87e9-4e63-935e-8debb7c6cc86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When we use map() with a Pair RDD, we get access to both the key & value. Often we are only interested in accessing the value (and not key). In those cases, we can use mapValues() instead of map() to apply a function. Here I use the 'sorted' function and apply it to the grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71450d75-621b-41d7-819e-b030a4266488",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sortedRDD = groupRDD.mapValues(lambda x: sorted(x))\n",
    "\n",
    "#What is the key for the first group\n",
    "print(sortedRDD.collect()[0][0])\n",
    "\n",
    "#What are the values in the first group\n",
    "for i in sortedRDD.collect()[0][1]:\n",
    "  print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02d12731-3554-4e53-ba34-a70dbc6ad86f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Use mapValues on the paired_RDD to round values to the nearest euro in a new RDD. Python has a 'round' function which can be useful for you here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6982b83-6836-4813-bf58-06684a9121f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a384c0-7def-4958-88bd-8bccf92047a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use reduce to find the highest sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4513ac1-abbc-46f4-982d-4ff57bfee47f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extra explanation below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b41f14-1a34-441d-ac0f-40a896538618",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use reduceByKey to find the highest sale by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee2e9513-9285-4a77-b0b3-1f02649138c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4eab08a-9c70-4ccc-9f74-4aec9c4d41b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Do you notice the difference when fetching the results?  \n",
    "If you look at the cheatsheet provided you can see one is a transformation and the other is and action"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab4_bda_rdd_class",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
